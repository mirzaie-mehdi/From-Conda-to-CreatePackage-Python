{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ecaqr9YkWpbL"
   },
   "source": [
    "# Multilayer Perceptron (MLP) From Scratch\n",
    "\n",
    "\n",
    "\n",
    "In this section, we develop the fundamental components of a feedforward neural network from first principles. Each component is derived mathematically and implemented explicitly using NumPy. No automatic differentiation or high-level abstractions are used; all gradients are computed manually to make the learning dynamics fully transparent.\n",
    "\n",
    "We begin by defining a common interface for network layers, then implement linear transformations and nonlinear activation functions. Together, these components form the basis of forward propagation and backpropagation.\n",
    "## Mathematical Equations, Computational Graphs, and Backpropagation\n",
    "\n",
    "This notebook builds an MLP by defining **Layer** objects (Dense, ReLU, etc.) and composing them.\n",
    "\n",
    "We cover:\n",
    "- Forward equations\n",
    "- Backward gradient equations\n",
    "- Computational graph intuition\n",
    "- Softmax + cross-entropy loss\n",
    "- A minimal training loop\n",
    "\n",
    "## 1. Notation and Setup\n",
    "\n",
    "We use mini-batches.\n",
    "\n",
    "- Batch size: \\(N\\)\n",
    "- Input dimension: \\(d_{in}\\)\n",
    "- Hidden dimension: \\(d_h\\)\n",
    "- Output classes: \\(C\\)\n",
    "\n",
    "A layer maps:\n",
    "\n",
    "$\\mathbf{y} = f(\\mathbf{x}; \\theta)$\n",
    "\n",
    "The MLP is a composition:\n",
    "\n",
    "$f(\\mathbf{x}) = f_L \\circ f_{L-1} \\circ \\dots \\circ f_1(\\mathbf{x})$\n",
    "\n",
    "Backpropagation is reverse-mode differentiation on the computational graph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dense (Fully Connected) Layer\n",
    "\n",
    "### Forward\n",
    "For a batch input \n",
    "$\\mathbf{x} \\in \\mathbb{R}^{N \\times d_{in}}$:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{x}\\mathbf{W} + \\mathbf{b}$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\mathbf{W} \\in \\mathbb{R}^{d_{in} \\times d_{out}}$\n",
    "- $\\mathbf{b} \\in \\mathbb{R}^{d_{out}}$\n",
    "- $\\mathbf{y} \\in \\mathbb{R}^{N \\times d_{out}}$\n",
    "---\n",
    "### Computational graph (Dense)\n",
    "\n",
    "```\n",
    "x ──┐\n",
    "    |\n",
    "├── MatMul ── z ── Add ── y\n",
    "    |               ▲\n",
    "W ──┘               │\n",
    "                    b\n",
    "```\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$$ z = x W $$\n",
    "$$ y = z + b $$\n",
    "\n",
    "---\n",
    "\n",
    "### Backward (gradients)\n",
    "Assume upstream gradient:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{y}} = \\mathbf{G}$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{W}} = \\mathbf{x}^T \\mathbf{G}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{b}} = \\sum_{n=1}^{N} \\mathbf{G}_n$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{x}} = \\mathbf{G}\\mathbf{W}^T$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def forward(self, x, training=True):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, grad_out):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def params(self):\n",
    "        return []\n",
    "\n",
    "    def grads(self):\n",
    "        return []\n",
    "\n",
    "    def zero_grads(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, in_features, out_features, weight_scale=0.01):\n",
    "        self.W = weight_scale * np.random.randn(in_features, out_features)\n",
    "        self.b = np.zeros(out_features)\n",
    "        self.x = None  # cache\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        self.x = x\n",
    "        return x @ self.W + self.b\n",
    "\n",
    "    def backward(self, grad_out):\n",
    "        # grad_out = dL/dy\n",
    "        self.dW = self.x.T @ grad_out           # dL/dW\n",
    "        self.db = grad_out.sum(axis=0)          # dL/db\n",
    "        grad_x = grad_out @ self.W.T            # dL/dx\n",
    "        return grad_x\n",
    "\n",
    "    def params(self):\n",
    "        return [self.W, self.b]\n",
    "\n",
    "    def grads(self):\n",
    "        return [self.dW, self.db]\n",
    "\n",
    "    def zero_grads(self):\n",
    "        self.dW.fill(0.0)\n",
    "        self.db.fill(0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ReLU(Rectified Linear Unit) Activation\n",
    "\n",
    "### Forward\n",
    "\n",
    "- The ReLU function outputs 0 if the input is less than zero, otherwise it will return the input itself:\n",
    "$$f(x) = \\text{ReLU}(x) = \\max(0,x) $$\n",
    "\n",
    "- ReLU helps introduce non-linearity into the model, which is essential for learning complex patterns.\n",
    "- It also helps avoid the vanishing gradient problem common in deep networks with the Sigmoid activation.\n",
    "- During backpropagation, only the gradients for inputs greater than 0 pass through:\n",
    "\n",
    "$$ f'(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ 0 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "\n",
    "### Computational graph (Dense)\n",
    "\n",
    "```\n",
    "x ── ReLU ── y\n",
    "    \n",
    "```\n",
    "\n",
    "### Backward\n",
    "The derivative is:\n",
    "\n",
    "$$\\frac{d}{dx}\\text{ReLU}(x) =\n",
    "\\begin{cases}\n",
    "1 & x > 0 \\\\\n",
    "0 & x \\le 0\n",
    "\\end{cases}$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\odot \\mathbb{1}_{x>0}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        self.mask = (x > 0)\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, grad_out):\n",
    "        return grad_out * self.mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building an MLP by Composing Layers (Sequential)\n",
    "\n",
    "If we build:\n",
    "\n",
    "$$\\mathbf{x} \\rightarrow \\text{Dense}_1 \\rightarrow \\text{ReLU} \\rightarrow \\text{Dense}_2 \\rightarrow \\text{ReLU} \\rightarrow \\text{Dense}_3 \\rightarrow \\mathbf{z}$$\n",
    "\n",
    "Then the computational graph is:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Layer):\n",
    "    def __init__(self, *layers):\n",
    "        self.layers = list(layers)\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x, training=training)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad_out):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_out = layer.backward(grad_out)\n",
    "        return grad_out\n",
    "\n",
    "    def params(self):\n",
    "        ps = []\n",
    "        for layer in self.layers:\n",
    "            ps.extend(layer.params())\n",
    "        return ps\n",
    "\n",
    "    def grads(self):\n",
    "        gs = []\n",
    "        for layer in self.layers:\n",
    "            gs.extend(layer.grads())\n",
    "        return gs\n",
    "\n",
    "    def zero_grads(self):\n",
    "        for layer in self.layers:\n",
    "            layer.zero_grads()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Softmax + Cross-Entropy Loss\n",
    "\n",
    "### Softmax (probabilities)\n",
    "Given logits \n",
    "$\\mathbf{z} \\in \\mathbb{R}^{N \\times C}$:\n",
    "$$p_{n,i} = \\frac{e^{z_{n,i}}}{\\sum_{j=1}^{C} e^{z_{n,j}}}$$\n",
    "\n",
    "\n",
    "### Cross-entropy (integer class labels)\n",
    "If the true class for sample \\(n\\) is \\(y_n\\):\n",
    "\n",
    "$$L = \\frac{1}{N}\\sum_{n=1}^{N} -\\log(p_{n,y_n})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_with_logits(logits, y):\n",
    "    \"\"\"\n",
    "    logits: (N, C)\n",
    "    y: (N,) integer labels\n",
    "    returns: (loss, dlogits)\n",
    "    \"\"\"\n",
    "    N, C = logits.shape\n",
    "\n",
    "    # stable softmax\n",
    "    z = logits - logits.max(axis=1, keepdims=True)\n",
    "    exp_z = np.exp(z)\n",
    "    probs = exp_z / exp_z.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # loss\n",
    "    loss = -np.log(probs[np.arange(N), y] + 1e-12).mean()\n",
    "\n",
    "    # gradient: (p - y_onehot)/N\n",
    "    dlogits = probs.copy()\n",
    "    dlogits[np.arange(N), y] -= 1.0\n",
    "    dlogits /= N\n",
    "\n",
    "    return loss, dlogits\n",
    "\n",
    "\n",
    "def accuracy_from_logits(logits, y):\n",
    "    pred = np.argmax(logits, axis=1)\n",
    "    return (pred == y).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Parameter Update (SGD)\n",
    "\n",
    "Gradient descent update:\n",
    "\n",
    "$$\\theta \\leftarrow \\theta - \\eta \\frac{\\partial L}{\\partial \\theta}$$\n",
    "\n",
    "We apply this to each parameter tensor in the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=1e-2, weight_decay=0.0):\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def step(self, model):\n",
    "        params = model.params()\n",
    "        grads = model.grads()\n",
    "\n",
    "        for p, g in zip(params, grads):\n",
    "            if self.weight_decay != 0.0:\n",
    "                g = g + self.weight_decay * p\n",
    "            p -= self.lr * g\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Toy Dataset (for demonstration)\n",
    "\n",
    "We’ll generate a simple 2D classification dataset.\n",
    "\n",
    "This is not about dataset quality—just to verify:\n",
    "- forward works\n",
    "- backward gradients propagate\n",
    "- loss decreases during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(0)\n",
    "\n",
    "N = 400\n",
    "X = np.random.randn(N, 2)\n",
    "\n",
    "# non-linear target (quadrant-based)\n",
    "y = (X[:, 0] * X[:, 1] > 0).astype(int)  # 0 or 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Build and Train the MLP\n",
    "\n",
    "Model:\n",
    "\n",
    "$$\\mathbf{x} \\rightarrow \\text{Dense}(2, 64) \\rightarrow \\text{ReLU} \\rightarrow \\text{Dense}(64, 64) \\rightarrow \\text{ReLU} \\rightarrow \\text{Dense}(64, 2)$$\n",
    "\n",
    "Training loop:\n",
    "1. Forward: logits\n",
    "2. Loss + gradient: $L, \\frac{\\partial L}{\\partial \\mathbf{z}}$\n",
    "3. Backward through model\n",
    "4. SGD update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 50  loss=0.6870  acc=0.655\n",
      "epoch=100  loss=0.6803  acc=0.685\n",
      "epoch=150  loss=0.6734  acc=0.685\n",
      "epoch=200  loss=0.6651  acc=0.698\n",
      "epoch=250  loss=0.6565  acc=0.710\n",
      "epoch=300  loss=0.6478  acc=0.720\n"
     ]
    }
   ],
   "source": [
    "set_seed(0)\n",
    "\n",
    "model = Sequential(\n",
    "    Dense(2, 64, weight_scale=0.1),\n",
    "    ReLU(),\n",
    "    Dense(64, 64, weight_scale=0.1),\n",
    "    ReLU(),\n",
    "    Dense(64, 2, weight_scale=0.1),\n",
    ")\n",
    "\n",
    "opt = SGD(lr=1e-2, weight_decay=1e-4)\n",
    "\n",
    "for epoch in range(1, 301):\n",
    "    # forward\n",
    "    logits = model.forward(X, training=True)\n",
    "\n",
    "    # loss + initial gradient\n",
    "    loss, dlogits = softmax_cross_entropy_with_logits(logits, y)\n",
    "\n",
    "    # backward\n",
    "    model.zero_grads()\n",
    "    model.backward(dlogits)\n",
    "\n",
    "    # update\n",
    "    opt.step(model)\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        acc = accuracy_from_logits(model.forward(X, training=False), y)\n",
    "        print(f\"epoch={epoch:3d}  loss={loss:.4f}  acc={acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Student Checklist (Understanding Backprop)\n",
    "\n",
    "Students should be able to explain:\n",
    "\n",
    "1. Why Dense backward gives:\n",
    "   $$\\frac{\\partial L}{\\partial \\mathbf{W}} = \\mathbf{x}^T \\frac{\\partial L}{\\partial \\mathbf{y}}$$\n",
    "2. Why ReLU backward uses a mask:\n",
    "   \n",
    "   $$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y}\\odot \\mathbb{1}_{x>0}$$\n",
    "   \n",
    "3. Why softmax + cross-entropy gradient simplifies to:\n",
    "   \n",
    "   $$\\frac{\\partial L}{\\partial \\mathbf{z}} = \\frac{1}{N}(\\mathbf{p}-\\mathbf{Y})$$\n",
    "   \n",
    "4. How the computational graph explains the chain rule:\n",
    "   \n",
    "   $$\\frac{\\partial L}{\\partial u} = \\sum_{v} \\frac{\\partial L}{\\partial v}\\frac{\\partial v}{\\partial u}$$\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation and Gradient Flow in a Multilayer Perceptron\n",
    "\n",
    "This section provides a consolidated view of how gradients flow through the network,\n",
    "from the loss function back to each layer’s parameters.\n",
    "\n",
    "## Gradient Flow Diagram (Forward and Backward)\n",
    "\n",
    "### Forward pass\n",
    "\n",
    "```\n",
    "                                        X (input batch)\n",
    "                                           │\n",
    "                                           ▼\n",
    "                                        Dense1: Z1 = XW1 + b1\n",
    "                                           │\n",
    "                                           ▼\n",
    "                                        ReLU1:  A1 = max(0, Z1)\n",
    "                                           │\n",
    "                                           ▼\n",
    "                                        Dense2: Z2 = A1W2 + b2\n",
    "                                           │\n",
    "                                           ▼\n",
    "                                        ReLU2:  A2 = max(0, Z2)\n",
    "                                           │\n",
    "                                           ▼\n",
    "                                        Dense3: Z3 = A2W3 + b3   (logits)\n",
    "                                           │\n",
    "                                           ▼\n",
    "                                        Softmax + Cross-Entropy Loss L\n",
    "````\n",
    "\n",
    "### Backward pass (reverse order)\n",
    "\n",
    "```\n",
    "\n",
    "                                        ∂L/∂Z3  (from loss)\n",
    "                                           │\n",
    "                                           ├── ∂L/∂W3 , ∂L/∂b3\n",
    "                                           ▼\n",
    "                                        ∂L/∂A2 = ∂L/∂Z3 · W3ᵀ\n",
    "                                           │\n",
    "                                           ▼\n",
    "                                        ReLU mask (Z2 > 0)\n",
    "                                           │\n",
    "                                           ▼\n",
    "                                        ∂L/∂Z2\n",
    "                                           │\n",
    "                                           ├── ∂L/∂W2 , ∂L/∂b2\n",
    "                                           ▼\n",
    "                                        ∂L/∂A1 = ∂L/∂Z2 · W2ᵀ\n",
    "                                           │\n",
    "                                           ▼\n",
    "                                        ReLU mask (Z1 > 0)\n",
    "                                           │\n",
    "                                           ▼\n",
    "                                        ∂L/∂Z1\n",
    "                                           │\n",
    "                                           ├── ∂L/∂W1 , ∂L/∂b1\n",
    "                                           ▼\n",
    "                                        ∂L/∂X\n",
    "\n",
    "```\n",
    "\n",
    "## Consolidated Backpropagation Mathematics\n",
    "\n",
    "Consider a minibatch:\n",
    "- Input: $ X \\in \\mathbb{R}^{N \\times d_{in}} $\n",
    "- Labels: $ y \\in \\{0,\\dots,C-1\\}^N $\n",
    "\n",
    "The MLP is defined as:\n",
    "\n",
    "$\\begin{aligned}\n",
    "Z_1 &= XW_1 + b_1 \\\\\n",
    "A_1 &= \\mathrm{ReLU}(Z_1) \\\\\n",
    "Z_2 &= A_1W_2 + b_2 \\\\\n",
    "A_2 &= \\mathrm{ReLU}(Z_2) \\\\\n",
    "Z_3 &= A_2W_3 + b_3\n",
    "\\end{aligned}$\n",
    "\n",
    "### Step 1: Softmax + Cross-Entropy Gradient\n",
    "\n",
    "\n",
    "$P = \\mathrm{softmax}(Z_3)$\n",
    "\n",
    "\n",
    "$L = -\\frac{1}{N}\\sum_{n=1}^N \\log P_{n,y_n}$\n",
    "\n",
    "The gradient with respect to logits is:\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial Z_3} = \\frac{1}{N}(P - Y)$\n",
    "\n",
    "This gradient is the **starting point of backpropagation**.\n",
    "\n",
    "### Step 2: Dense Layer Backpropagation\n",
    "\n",
    "For a dense layer:\n",
    "\n",
    "$Z = AW + b$\n",
    "\n",
    "Given upstream gradient $G = \\frac{\\partial L}{\\partial Z} $:\n",
    "\n",
    "\n",
    "$\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial W} &= A^\\top G \\\\\n",
    "\\frac{\\partial L}{\\partial b} &= \\sum_{n=1}^N G_{n,:} \\\\\n",
    "\\frac{\\partial L}{\\partial A} &= G W^\\top\n",
    "\\end{aligned}$\n",
    "\n",
    "This applies to **all Dense layers** in the network.\n",
    "\n",
    "### Step 3: ReLU Backpropagation\n",
    "\n",
    "$A = \\mathrm{ReLU}(Z)$\n",
    "\n",
    "The derivative of ReLU is:\n",
    "\n",
    "$\\frac{\\partial A}{\\partial Z} =\n",
    "\\begin{cases}\n",
    "1 & Z > 0 \\\\\n",
    "0 & Z \\le 0\n",
    "\\end{cases}$\n",
    "\n",
    "Thus:\n",
    "$\\frac{\\partial L}{\\partial Z} =\n",
    "\\frac{\\partial L}{\\partial A} \\odot \\mathbf{1}[Z > 0]$\n",
    "\n",
    "ReLU has **no parameters**, but it gates gradient flow.\n",
    "\n",
    "## End-to-End Backpropagation Summary\n",
    "\n",
    "- The loss function provides $ \\frac{\\partial L}{\\partial Z_3} $\n",
    "- Each Dense layer computes gradients for its **weights, bias, and inputs**\n",
    "- Each ReLU layer applies a binary mask\n",
    "- Gradients propagate backward via the chain rule\n",
    "- The optimizer updates parameters using stored gradients\n",
    "\n",
    "This mirrors exactly how the `backward()` methods are composed in the code.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Ecaqr9YkWpbL"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1179f322ebf643fcbc389efa1e80f0d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DropdownModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DropdownModel",
      "_options_labels": [
       "16",
       "32",
       "64",
       "128",
       "256"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "DropdownView",
      "description": "Batch Size:",
      "description_tooltip": null,
      "disabled": false,
      "index": 4,
      "layout": "IPY_MODEL_b9ab8195f2d94405a822ea67ea1fd094",
      "style": "IPY_MODEL_48a192d8f8754cac918a4ea1fa60e23a"
     }
    },
    "48a192d8f8754cac918a4ea1fa60e23a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b9ab8195f2d94405a822ea67ea1fd094": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "200px"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

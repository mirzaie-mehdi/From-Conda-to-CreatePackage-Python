{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ecaqr9YkWpbL"
   },
   "source": [
    "# Neural Networks from Scratch\n",
    "\n",
    "\n",
    "\n",
    "In this section, we build and train a neural network from first principles using the NumPy library. The objective is to understand the core mechanics of neural networks—forward propagation, loss computation, and parameter updates—without relying on high-level frameworks. By implementing each component manually, we gain insight into how neural networks operate under the hood.\n",
    "\n",
    "In the following section, we transition to the PyTorch framework and reimplement the same neural network using its abstractions. This comparison highlights both the underlying similarities and the practical advantages of modern deep learning libraries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "PqFOwWLZVPI0"
   },
   "outputs": [],
   "source": [
    "# @title setup and imports\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import trange\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "9AKM0RgiVSOQ"
   },
   "outputs": [],
   "source": [
    "# @title helper functions\n",
    "\n",
    "def plot_training(losses):\n",
    "    # Plot the loss\n",
    "    plt.plot(losses)\n",
    "    plt.title(\"Training loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names, kept_classes):\n",
    "    dim = len(kept_classes)\n",
    "    labels = [class_names[i] for i in kept_classes]\n",
    "    # Plot the confusion matrix\n",
    "    conf_mat = confusion_matrix(y_true, y_pred)\n",
    "    norm_conf_mat = conf_mat / np.sum(conf_mat, axis=1)\n",
    "    # plot the matrix\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.imshow(norm_conf_mat)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predictions')\n",
    "    plt.ylabel('Labels')\n",
    "    plt.xticks(range(dim), labels, rotation=45)\n",
    "    plt.yticks(range(dim), labels)\n",
    "    plt.colorbar()\n",
    "    # Put number of each cell in plot\n",
    "    for i in range(dim):\n",
    "        for j in range(dim):\n",
    "            c = conf_mat[j, i]\n",
    "            color = 'black' if c > 500 else 'white'\n",
    "            ax.text(i, j, str(int(c)), va='center', ha='center', color=color)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_data(filter_classes):\n",
    "    fashion_mnist = fetch_openml(\"Fashion-MNIST\", parser='auto')\n",
    "    x, y = fashion_mnist['data'], fashion_mnist['target'].astype(int)\n",
    "    # Remove classes\n",
    "    filtered_indices = np.isin(y, filter_classes)\n",
    "    x, y = x[filtered_indices].to_numpy(), y[filtered_indices]\n",
    "    # Normalize the pixels to be in [-1, +1] range\n",
    "    x = ((x / 255.) - .5) * 2\n",
    "    removed_class_count = 0\n",
    "    for i in range(10):  # Fix the labels\n",
    "        if i in filter_classes and removed_class_count != 0:\n",
    "            y[y == i] = i - removed_class_count\n",
    "        elif i not in filter_classes:\n",
    "            removed_class_count += 1\n",
    "    # Do the train-test split\n",
    "    return train_test_split(x, y, test_size=10_000)\n",
    "\n",
    "\n",
    "def onehot_encoder(y, num_labels):\n",
    "    one_hot = np.zeros(shape=(y.size, num_labels), dtype=int)\n",
    "    one_hot[np.arange(y.size), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def plot_batch_size(vanila, stochastic, mini_batch):\n",
    "    fig, axes = plt.subplots(2, 2)\n",
    "    # Plot the loss\n",
    "    axes[0, 0].plot(vanila[0], label='Gradient Descent')\n",
    "    axes[0, 0].plot(stochastic[0], label='Stochastic Gradient Descent')\n",
    "    axes[0, 0].plot(mini_batch[0], label='Mini-Batch Gradient Descent')\n",
    "    axes[0, 0].set_xlabel('Epoch'), axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Training Loss'), axes[0, 0].legend()\n",
    "    # Plot the accuracy\n",
    "    axes[0, 1].plot(vanila[2], label='Gradient Descent')\n",
    "    axes[0, 1].plot(stochastic[2], label='Stochastic Gradient Descent')\n",
    "    axes[0, 1].plot(mini_batch[2], label='Mini-Batch Gradient Descent')\n",
    "    axes[0, 1].set_xlabel('Epoch'), axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].set_title('Test Accuracy'), axes[0, 1].legend()\n",
    "    # Plot SGD batch loss\n",
    "    axes[1, 0].plot(stochastic[1], label='Stochastic Gradient Descent')\n",
    "    axes[1, 0].set_xlabel('Batch'), axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].set_title('Stochastic Gradient Descent')\n",
    "    # Plot MBGD batch loss\n",
    "    axes[1, 1].plot(mini_batch[1], label='Mini-Batch Gradient Descent')\n",
    "    axes[1, 1].set_xlabel('Batch'), axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].set_title('Mini-Batch Gradient Descent')\n",
    "\n",
    "    fig.set_size_inches(16, 12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvFUeXMNadnZ"
   },
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KThKuK5DJHAH"
   },
   "source": [
    "## Abstract Layer Class\n",
    "\n",
    "The `Layer` class serves as an abstract base class for all layers in the network. It includes placeholder methods:\n",
    "- `forward`: The forward pass computes the output of the layer given an input.\n",
    "- `backward`: The backward pass computes the gradients with respect to the input and parameters.\n",
    "- `step`: Updates the layer parameters (weights and biases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5HmznEXldEaI"
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.inp = None\n",
    "        self.out = None\n",
    "\n",
    "    def __call__(self, inp: np.ndarray) -> np.ndarray:\n",
    "        return self.forward(inp)\n",
    "\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def step(self, lr: float) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eB1SjTeaJztY"
   },
   "source": [
    "## Linear Layers\n",
    "\n",
    "The `Linear` class implements the fully connected (or dense) layer of a neural network, which performs a linear transformation on the input:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{x} \\cdot \\mathbf{W} + \\mathbf{b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phTfqRsVIZKA"
   },
   "source": [
    "**Initialization**\n",
    "- `self.w`: Represents the weight matrix of shape `(in_dim, out_dim)`, initialized using small random values.\n",
    "- `self.b`: Bias vector of shape `(1, out_dim)`, initialized to zeros.\n",
    "- `self.dw` and `self.db`: These store the computed gradients of weights and biases during backpropagation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Forward Pass**\n",
    "- The forward pass computes:\n",
    "$$\\mathbf{out} = \\mathbf{inp} \\cdot \\mathbf{W} + \\mathbf{b}$$\n",
    "where:\n",
    "  - `inp`: Input matrix of shape `(batch_size, in_dim)`\n",
    "  - `self.w`: Weight matrix of shape `(in_dim, out_dim)`\n",
    "  -\t`self.b`: Bias matrix of shape `(1, out_dim)`\n",
    "-\tThe result is a matrix out of shape `(batch_size, out_dim)`.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Backward Pass**\n",
    "- The backward pass computes gradients needed for updating the weights and biases. Given the upstream gradient `up_grad` (from the loss with respect to the output of this layer), we calculate:\n",
    "  - Gradient w.r.t. weights (`self.dw`):\n",
    "    $$ \\frac{\\partial L}{\\partial W} = \\mathbf{inp}^T \\cdot \\text{up_grad} $$\n",
    "  - Gradient w.r.t. biases (`self.db`):\n",
    "    $$\\frac{\\partial L}{\\partial b} = \\sum \\text{up_grad} \\text{ (summed across batch)}$$\n",
    "  - Gradient to propagate to the previous layer (`down_grad`):\n",
    "    $$\\text{down_grad} = \\text{up_grad} \\cdot W^T$$\n",
    "- This allows the gradient to flow backward to earlier layers.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Step Method**\n",
    "- Updates the weights and biases using the computed gradients and learning rate (`lr`):\n",
    "    $$W = W - lr \\cdot \\frac{\\partial L}{\\partial W}$$\n",
    "    $$b = b - lr \\cdot \\frac{\\partial L}{\\partial b}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pF0DKJV_JRnh"
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        # He initialization: better scaling for deep networks\n",
    "        self.w = 0.1 * np.random.randn(in_dim, out_dim)\n",
    "        self.b = np.zeros((1, out_dim))\n",
    "        self.dw = np.zeros_like(self.w)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Perform the linear transformation: output = inp * W + b\"\"\"\n",
    "        self.inp = inp\n",
    "        self.out = np.dot(inp, self.w) + self.b\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backpropagate the gradients through this layer.\"\"\"\n",
    "        # Compute gradients for weights and biases\n",
    "        self.dw = np.dot(self.inp.T, up_grad)  # Gradient wrt weights\n",
    "        self.db = np.sum(up_grad, axis=0, keepdims=True)  # Gradient wrt biases\n",
    "        # Compute gradient to propagate back (downstream)\n",
    "        down_grad = np.dot(up_grad, self.w.T)\n",
    "        return down_grad\n",
    "\n",
    "    def step(self, lr: float) -> None:\n",
    "        \"\"\"Update the weights and biases using the gradients.\"\"\"\n",
    "        self.w -= lr * self.dw\n",
    "        self.b -= lr * self.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yr6hBcY4N00B"
   },
   "source": [
    "## Activation Functions\n",
    "\n",
    "We can implement activation functions as layers. This will simplify the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsuMui_fLCWf"
   },
   "source": [
    "### Sigmoid\n",
    "\n",
    "- The Sigmoid function is defined as follows:\n",
    "\n",
    "$$f(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "- Sigmoid squashes the input into the range [0, 1], making it useful for binary classification tasks.\n",
    "- It converts any real-valued number into a probability-like output.\n",
    "- However, in deeper networks, it may cause vanishing gradients due to its flat slope for extreme values.\n",
    "- The derivative of Sigmoid is convenient to compute using its output  $f(x)$:\n",
    "$$f'(x) = \\frac{-e^{-x}}{(1 + e^{-x})^2} = \\frac{1}{1 + e^{-x}} \\cdot \\frac{e^{-x}}{1 + e^{-x}} = f(x) \\cdot (1-f(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cxGzqMybEGt-"
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Sigmoid Activation: f(x) = 1 / (1 + exp(-x))\"\"\"\n",
    "        self.out = 1 / (1 + np.exp(-inp))\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for Sigmoid: f'(x) = f(x) * (1 - f(x))\"\"\"\n",
    "        down_grad = self.out * (1 - self.out) * up_grad\n",
    "        return down_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08aibrACJ_kr"
   },
   "source": [
    "### ReLU (Rectified Linear Unit)\n",
    "\n",
    "- The ReLU function outputs 0 if the input is less than zero, otherwise it will return the input itself:\n",
    "$$f(x) = \\max(0,x) $$\n",
    "\n",
    "- ReLU helps introduce non-linearity into the model, which is essential for learning complex patterns.\n",
    "- It also helps avoid the vanishing gradient problem common in deep networks with the Sigmoid activation.\n",
    "- During backpropagation, only the gradients for inputs greater than 0 pass through:\n",
    "\n",
    "$$ f'(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ 0 & \\text{otherwise} \\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dHaTiE10HART"
   },
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"ReLU Activation: f(x) = max(0, x)\"\"\"\n",
    "        self.inp = inp\n",
    "        self.out = np.maximum(0, inp)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for ReLU: derivative is 1 where input > 0, else 0.\"\"\"\n",
    "        down_grad = up_grad * (self.inp > 0)  # Efficient boolean indexing\n",
    "        return down_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CaQZUpp1coQ1"
   },
   "source": [
    "### Softmax\n",
    "\n",
    "- The Softmax function is defined as follows:\n",
    "$$f(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n",
    "\n",
    "- Softmax normalizes the input values into probabilities that sum to 1.\n",
    "- It's typically used in the final layer of a neural network for multi-class classification.\n",
    "- It converts raw scores into probabilities, where each class has a non-negative probability between 0 and 1.\n",
    "- Subtracting the maximum input value (`np.max(inp)`) from all inputs before applying `np.exp` helps prevent overflow errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wrq1L5ZCO0vM"
   },
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Softmax Activation: f(x) = exp(x) / sum(exp(x))\"\"\"\n",
    "        # Subtract max for numerical stability\n",
    "        exp_values = np.exp(inp - np.max(inp, axis=1, keepdims=True))\n",
    "        self.out = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for Softmax using the Jacobian matrix.\"\"\"\n",
    "        down_grad = np.empty_like(up_grad)\n",
    "        for i in range(up_grad.shape[0]):\n",
    "            single_output = self.out[i].reshape(-1, 1)\n",
    "            jacobian = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            down_grad[i] = np.dot(jacobian, up_grad[i])\n",
    "        return down_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sl2nW4KnSgYs"
   },
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyEmTPR-TLDk"
   },
   "source": [
    "## Abstract Loss Class\n",
    "\n",
    "The `Loss` class serves as an abstract base class for all layers in the network. It includes placeholder methods:\n",
    "- `forward`: To compute the loss given predictions and targets.\n",
    "- `backward`: To compute the loss given predictions and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qMb2DsJNSp3f"
   },
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def __init__(self):\n",
    "        self.prediction = None\n",
    "        self.target = None\n",
    "        self.loss = None\n",
    "\n",
    "    def __call__(self, prediction: np.ndarray, target: np.ndarray) -> float:\n",
    "        return self.forward(prediction, target)\n",
    "\n",
    "    def forward(self, prediction: np.ndarray, target: np.ndarray) -> float:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self) -> np.ndarray:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__nX-zlJTo3B"
   },
   "source": [
    "## Cross-Entropy Loss\n",
    "\n",
    "Cross-entropy loss is typically used in classification tasks since it measures the dissimilarity between the true distribution (target) and the predicted probability distribution (prediction):\n",
    "\n",
    "$$L = - \\frac{1}{N} \\sum_{i} \\sum_{c} y_{ic} \\log(p_{ic})$$\n",
    "\n",
    "where $y_{ic}$ is the one-hot encoded true label (target), $p_{ic}$ is the predicted probability (output from Softmax) and $N$ is the batch size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lyF-rhVvSy1q"
   },
   "outputs": [],
   "source": [
    "class CrossEntropy(Loss):\n",
    "    def forward(self, prediction: np.ndarray, target: np.ndarray) -> float:\n",
    "        \"\"\"Cross-Entropy Loss for classification.\"\"\"\n",
    "        self.prediction = prediction\n",
    "        self.target = target\n",
    "        # Clip predictions to avoid log(0)\n",
    "        clipped_pred = np.clip(prediction, 1e-12, 1.0)\n",
    "        # Compute and return the loss\n",
    "        self.loss = -np.mean(np.sum(target * np.log(clipped_pred), axis=1))\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self) -> np.ndarray:\n",
    "        \"\"\"Gradient of Cross-Entropy Loss.\"\"\"\n",
    "        # Gradient wrt prediction (assuming softmax and one-hot targets)\n",
    "        grad = -self.target / self.prediction / self.target.shape[0]\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-i8pHfR9TrYa"
   },
   "source": [
    "## Mean Squared Error (MSE) Loss\n",
    "\n",
    "MSE is used primarily for regression tasks, where you need to measure the distance between the predicted continuous values and the true values:\n",
    "\n",
    "$$L = \\frac{1}{N} \\sum_{i} (p_i - y_i)^2$$\n",
    "\n",
    "where $p_i$ is the predicted value, $y_i$ is the true value (target) and $N$ is the batch size.\n",
    "\n",
    "The gradient measures the difference between the prediction and the target, scaled by the batch size:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial p_i} = \\frac{2}{N} (p_i - y_i)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0ZmkwcRCSzRd"
   },
   "outputs": [],
   "source": [
    "class MSE(Loss):\n",
    "    def forward(self, prediction: np.ndarray, target: np.ndarray) -> float:\n",
    "        \"\"\"Mean Squared Error Loss for regression.\"\"\"\n",
    "        self.prediction = prediction\n",
    "        self.target = target\n",
    "        # Compute and return the loss\n",
    "        self.loss = np.mean((prediction - target) ** 2)\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self) -> np.ndarray:\n",
    "        \"\"\"Gradient of MSE Loss.\"\"\"\n",
    "        grad = 2 * (self.prediction - self.target) / self.target.size\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1KKCeDWUNJc"
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJno_CD7hK1u"
   },
   "source": [
    "Now we can combine everything we've done earlier to build a neural network class called `MLP` with the following methods:\n",
    "\n",
    "- `forward`: Sequentially passes input through each layer in the network to compute the output.\n",
    "- `loss`: Computes the loss between the predicted output and the true target using the specified loss function.\n",
    "- `backward`: Propagates the gradient from the loss function through each layer, updating the gradients of the parameters in each layer.\n",
    "- `update`: Updates each layer's parameters (e.g., weights and biases) using the gradients computed during backpropagation.\n",
    "- `train`: Executes the training loop for a specified number of epochs, iterating over the dataset in mini-batches, performing the forward pass, computing the loss, backpropagating the gradients, and updating the parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "nd1HjMLMUOAs"
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, layers: list[Layer], loss_fn: Loss, lr: float) -> None:\n",
    "        \"\"\"\n",
    "        Multi-Layer Perceptron (MLP) class.\n",
    "        Arguments:\n",
    "        - layers: List of layers (e.g., Linear, ReLU, etc.).\n",
    "        - loss_fn: Loss function object (e.g., CrossEntropy, MSE).\n",
    "        - lr: Learning rate.\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.loss_fn = loss_fn\n",
    "        self.lr = lr\n",
    "\n",
    "    def __call__(self, inp: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Makes the model callable, equivalent to forward pass.\"\"\"\n",
    "        return self.forward(inp)\n",
    "\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Pass input through each layer sequentially.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            inp = layer.forward(inp)\n",
    "        return inp\n",
    "\n",
    "    def loss(self, prediction: np.ndarray, target: np.ndarray) -> float:\n",
    "        \"\"\"Calculate the loss.\"\"\"\n",
    "        return self.loss_fn(prediction, target)\n",
    "\n",
    "    def backward(self) -> None:\n",
    "        \"\"\"Perform backpropagation by propagating the gradient backwards through the layers.\"\"\"\n",
    "        up_grad = self.loss_fn.backward()\n",
    "        for layer in reversed(self.layers):\n",
    "            up_grad = layer.backward(up_grad)\n",
    "\n",
    "    def update(self) -> None:\n",
    "        \"\"\"Update the parameters of each layer using the gradients and the learning rate.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.step(self.lr)\n",
    "\n",
    "    def train(self, x_train: np.ndarray, y_train: np.ndarray, epochs: int, batch_size: int) -> np.ndarray:\n",
    "        \"\"\"Train the MLP over the given dataset for a number of epochs.\"\"\"\n",
    "        losses = np.empty(epochs)\n",
    "        for epoch in (pbar := trange(epochs)):\n",
    "            running_loss = 0.0\n",
    "            for i in range(0, len(x_train), batch_size):\n",
    "                x_batch = x_train[i:i + batch_size]\n",
    "                y_batch = y_train[i:i + batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                prediction = self.forward(x_batch)\n",
    "\n",
    "                # Compute loss\n",
    "                running_loss += self.loss(prediction, y_batch) * batch_size\n",
    "\n",
    "                # Backward pass\n",
    "                self.backward()\n",
    "\n",
    "                # Update parameters\n",
    "                self.update()\n",
    "\n",
    "            # Normalize running loss by total number of samples\n",
    "            running_loss /= len(x_train)\n",
    "            pbar.set_description(f\"Loss: {running_loss:.3f}\")\n",
    "            losses[epoch] = running_loss\n",
    "\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzONLtl5kc8W"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r93pdZdrZ77B"
   },
   "source": [
    "## Loading the Fashion-MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOCW3Kh9azY6"
   },
   "source": [
    "For simplicity you can use `get_data` to load the Fashion-MNIST dataset. Since we aren't using GPUs, in order to save time and get better results, we are only going to include 3 classes in our training. However you can easily modify this cell to include different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xmRyKM-oZ-hh"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m kept_classes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m7\u001b[39m]  \u001b[38;5;66;03m# T-shirt/top, Trouser, Sneaker\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Download the dataset and split it into training and testing sets\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m x_train, x_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkept_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# One-hot encode the target labels of the training set\u001b[39;00m\n\u001b[1;32m     12\u001b[0m y_train \u001b[38;5;241m=\u001b[39m onehot_encoder(y_train, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(kept_classes))\n",
      "Cell \u001b[0;32mIn[2], line 37\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(filter_classes)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_data\u001b[39m(filter_classes):\n\u001b[0;32m---> 37\u001b[0m     fashion_mnist \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_openml\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFashion-MNIST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m fashion_mnist[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m], fashion_mnist[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Remove classes\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/jupyterbook/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:218\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    214\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    215\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    216\u001b[0m         )\n\u001b[1;32m    217\u001b[0m     ):\n\u001b[0;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    228\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/jupyterbook/lib/python3.10/site-packages/sklearn/datasets/_openml.py:1135\u001b[0m, in \u001b[0;36mfetch_openml\u001b[0;34m(name, version, data_id, data_home, target_column, cache, return_X_y, as_frame, n_retries, delay, parser, read_csv_kwargs)\u001b[0m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;66;03m# obtain the data\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m url \u001b[38;5;241m=\u001b[39m data_description[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m-> 1135\u001b[0m bunch \u001b[38;5;241m=\u001b[39m \u001b[43m_download_data_to_bunch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_home\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_frame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mas_frame\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopenml_columns_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmd5_checksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_description\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmd5_checksum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_csv_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_csv_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_X_y:\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bunch\u001b[38;5;241m.\u001b[39mdata, bunch\u001b[38;5;241m.\u001b[39mtarget\n",
      "File \u001b[0;32m~/miniforge3/envs/jupyterbook/lib/python3.10/site-packages/sklearn/datasets/_openml.py:689\u001b[0m, in \u001b[0;36m_download_data_to_bunch\u001b[0;34m(url, sparse, data_home, as_frame, openml_columns_info, data_columns, target_columns, shape, md5_checksum, n_retries, delay, parser, read_csv_kwargs)\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParserError\n\u001b[1;32m    687\u001b[0m     no_retry_exception \u001b[38;5;241m=\u001b[39m ParserError\n\u001b[0;32m--> 689\u001b[0m X, y, frame, categories \u001b[38;5;241m=\u001b[39m \u001b[43m_retry_with_clean_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_home\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_retry_exception\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_load_arff_response\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_home\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopenml_columns_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_names_to_select\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_names_to_select\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmd5_checksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmd5_checksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_csv_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_csv_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Bunch(\n\u001b[1;32m    707\u001b[0m     data\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m    708\u001b[0m     target\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    712\u001b[0m     target_names\u001b[38;5;241m=\u001b[39mtarget_columns,\n\u001b[1;32m    713\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/jupyterbook/lib/python3.10/site-packages/sklearn/datasets/_openml.py:66\u001b[0m, in \u001b[0;36m_retry_with_clean_cache.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m URLError:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/jupyterbook/lib/python3.10/site-packages/sklearn/datasets/_openml.py:524\u001b[0m, in \u001b[0;36m_load_arff_response\u001b[0;34m(url, data_home, parser, output_type, openml_columns_info, feature_names_to_select, target_names_to_select, shape, md5_checksum, n_retries, delay, read_csv_kwargs)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_load_arff_response\u001b[39m(\n\u001b[1;32m    442\u001b[0m     url: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    443\u001b[0m     data_home: Optional[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    453\u001b[0m     read_csv_kwargs: Optional[Dict] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    454\u001b[0m ):\n\u001b[1;32m    455\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load the ARFF data associated with the OpenML URL.\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \n\u001b[1;32m    457\u001b[0m \u001b[38;5;124;03m    In addition of loading the data, this function will also check the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;124;03m        `output_array_type == \"pandas\"`.\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 524\u001b[0m     gzip_file \u001b[38;5;241m=\u001b[39m \u001b[43m_open_openml_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_home\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_retries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m closing(gzip_file):\n\u001b[1;32m    526\u001b[0m         md5 \u001b[38;5;241m=\u001b[39m hashlib\u001b[38;5;241m.\u001b[39mmd5()\n",
      "File \u001b[0;32m~/miniforge3/envs/jupyterbook/lib/python3.10/site-packages/sklearn/datasets/_openml.py:188\u001b[0m, in \u001b[0;36m_open_openml_url\u001b[0;34m(url, data_home, n_retries, delay)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 opener \u001b[38;5;241m=\u001b[39m gzip\u001b[38;5;241m.\u001b[39mGzipFile\n\u001b[1;32m    187\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m opener(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(tmpdir, file_name), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fdst:\n\u001b[0;32m--> 188\u001b[0m                 \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopyfileobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfsrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfdst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m         shutil\u001b[38;5;241m.\u001b[39mmove(fdst\u001b[38;5;241m.\u001b[39mname, local_path)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/jupyterbook/lib/python3.10/shutil.py:195\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    193\u001b[0m fdst_write \u001b[38;5;241m=\u001b[39m fdst\u001b[38;5;241m.\u001b[39mwrite\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 195\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[43mfsrc_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m buf:\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/jupyterbook/lib/python3.10/http/client.py:460\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/jupyterbook/lib/python3.10/http/client.py:592\u001b[0m, in \u001b[0;36mHTTPResponse._read_chunked\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m=\u001b[39m chunk_left \u001b[38;5;241m-\u001b[39m amt\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 592\u001b[0m value\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_safe_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_left\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    594\u001b[0m     amt \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m chunk_left\n",
      "File \u001b[0;32m~/miniforge3/envs/jupyterbook/lib/python3.10/http/client.py:631\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_safe_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, amt):\n\u001b[1;32m    625\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read the number of bytes requested.\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \n\u001b[1;32m    627\u001b[0m \u001b[38;5;124;03m    This function should be used when <amt> bytes \"should\" be present for\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;124;03m    reading. If the bytes are truly not available (due to EOF), then the\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    IncompleteRead exception can be used to detect the problem.\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m<\u001b[39m amt:\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(data, amt\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(data))\n",
      "File \u001b[0;32m~/miniforge3/envs/jupyterbook/lib/python3.10/socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/jupyterbook/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniforge3/envs/jupyterbook/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class_names = {0: 'T-shirt/top', 1: 'Trouser', 2: 'Pullover',\n",
    "               3: 'Dress', 4: 'Coat', 5:  'Sandal', 6: 'Shirt',\n",
    "               7: 'Sneaker', 8: 'Bag', 9: 'Ankle boot'}\n",
    "\n",
    "# Include all the classes you want to see in training\n",
    "kept_classes = [0, 1, 7]  # T-shirt/top, Trouser, Sneaker\n",
    "\n",
    "# Download the dataset and split it into training and testing sets\n",
    "x_train, x_test, y_train, y_test = get_data(kept_classes)\n",
    "\n",
    "# One-hot encode the target labels of the training set\n",
    "y_train = onehot_encoder(y_train, num_labels=len(kept_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZeNgQXybi4P"
   },
   "source": [
    "## Training the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qfmP-hYkpfz"
   },
   "source": [
    "Now we can define the network and train it on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "eaIADoc8W7CS",
    "outputId": "7d762b34-7b46-461d-d183-7ee9ec1862e9"
   },
   "outputs": [],
   "source": [
    "# Define the layers of the neural network\n",
    "layers = [Linear(784, 50),\n",
    "          ReLU(),\n",
    "          Linear(50, 50),\n",
    "          ReLU(),\n",
    "          Linear(50, len(kept_classes)),\n",
    "          Softmax()]\n",
    "\n",
    "# Create the model\n",
    "model = MLP(layers, CrossEntropy(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "losses = model.train(x_train, y_train, epochs=30, batch_size=64)\n",
    "\n",
    "# Plot the training loss curve\n",
    "plot_training(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4Uh-ddyk2mR"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgO7eZ9Uk5X0"
   },
   "source": [
    "We can measure the models accuracy on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3M4fWNaZoHW",
    "outputId": "d8289ad2-2775-4293-9d8c-36f32ab0764a"
   },
   "outputs": [],
   "source": [
    "# Test the model\n",
    "y_prediction = np.argmax(model(x_test), axis=1)\n",
    "acc = 100 * np.mean(y_prediction == y_test)\n",
    "print(f'Test accuracy with {len(y_train)} training examples on {len(y_test)} test samples is {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2DrYQPilBlF"
   },
   "source": [
    "The confusion matrix can also be observed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "49LwgNLMj72R",
    "outputId": "a1ae708d-967a-4526-ccb1-83041fb594bb"
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "plot_confusion_matrix(y_test, y_prediction, class_names, kept_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KIRyFvYDkvd"
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-jahzvWEVV_"
   },
   "source": [
    "In this section we are going to run some experiments to better understand the different hyperparameters of our neural network.\n",
    "We will slightly modify the `MLP` class we wrote before to access different metrics during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "I9pWMq0yHnpV"
   },
   "outputs": [],
   "source": [
    "# @title Modified MLP\n",
    "\n",
    "class NN(MLP):\n",
    "    def test(self, x_test: np.ndarray, y_test: np.ndarray) -> float:\n",
    "        \"\"\"Calculate the test accuracy and return it.\"\"\"\n",
    "        y_pred = np.argmax(self.forward(x_test), axis=1)\n",
    "        return 100 * np.mean(y_pred == y_test)\n",
    "\n",
    "    def train(self, x_train: np.ndarray, y_train: np.ndarray, epochs: int,\n",
    "              batch_size: int, x_test: np.ndarray, y_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Train the MLP over the given dataset for a number of epochs.\"\"\"\n",
    "        losses = np.zeros(epochs)\n",
    "        batch_count = len(x_train) // batch_size\n",
    "        batch_losses = np.empty(epochs * batch_count + 1)\n",
    "        accuracies = np.empty(epochs)\n",
    "\n",
    "        for epoch in (pbar := trange(epochs)):\n",
    "            correct = 0\n",
    "            for i in range(0, len(x_train), batch_size):\n",
    "                x_batch = x_train[i:i + batch_size]\n",
    "                y_batch = y_train[i:i + batch_size]\n",
    "                # Forward pass\n",
    "                prediction = self.forward(x_batch)\n",
    "                # Compute loss\n",
    "                batch_losses[i // batch_size + epoch * batch_count] = self.loss(prediction, y_batch)\n",
    "                losses[epoch] += batch_losses[i // batch_size + epoch * batch_count]\n",
    "                # Backward pass\n",
    "                self.backward()\n",
    "                # Update parameters\n",
    "                self.update()\n",
    "\n",
    "            # Display and update the metrics\n",
    "            losses[epoch] /= batch_count\n",
    "            accuracies[epoch] = self.test(x_test, y_test)\n",
    "            pbar.set_description(f\"Train Loss = {losses[epoch]:.3f} | Test Accuracy = {accuracies[epoch]:.2f}% \")\n",
    "\n",
    "        return losses, batch_losses, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KcAcyeuDuO7"
   },
   "source": [
    "## Batch Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2A_sjspETKn"
   },
   "source": [
    "Here we will take a look at different batch sizes and how they effect training and convergence. Run the widget bellow to train the model for different batch sizes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "1179f322ebf643fcbc389efa1e80f0d9",
      "b9ab8195f2d94405a822ea67ea1fd094",
      "48a192d8f8754cac918a4ea1fa60e23a"
     ]
    },
    "id": "i9JQZLyCFiG6",
    "outputId": "2c68cb95-cb03-46ce-b31a-643c9e922f02"
   },
   "outputs": [],
   "source": [
    "# @markdown Batch Size Experimentation Widget\n",
    "\n",
    "class_names = {0: 'T-shirt/top', 1: 'Trouser', 2: 'Pullover',\n",
    "               3: 'Dress', 4: 'Coat', 5:  'Sandal', 6: 'Shirt',\n",
    "               7: 'Sneaker', 8: 'Bag', 9: 'Ankle boot'}\n",
    "\n",
    "# Include all the classes you want to see in training\n",
    "kept_classes = [0, 1, 7]  # T-shirt/top, Trouser, Sneaker\n",
    "\n",
    "# Download the dataset and split it into training and testing sets\n",
    "x_train, x_test, y_train, y_test = get_data(kept_classes)\n",
    "\n",
    "# One-hot encode the target labels of the training set\n",
    "y_train = onehot_encoder(y_train, num_labels=len(kept_classes))\n",
    "\n",
    "# Create a list of values\n",
    "options = [16, 32, 64, 128, 256]\n",
    "\n",
    "# Create a dropdown widget with custom layout\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=options,\n",
    "    description='Batch Size:',\n",
    "    layout={'width': '200px'},\n",
    ")\n",
    "\n",
    "# Define a function to run based on selected value\n",
    "def on_value_change(change):\n",
    "    mini_batch_size = change['new']\n",
    "    global first_run, gd, sgd, mbgd\n",
    "    if first_run:\n",
    "        # Gradient Descent\n",
    "        layers = [Linear(784, 50), ReLU(), Linear(50, 50), ReLU(), Linear(50, len(kept_classes)), Softmax()]\n",
    "        model = NN(layers, CrossEntropy(), lr=0.001)\n",
    "        print('Gradient Descent:', end='\\n\\t')\n",
    "        gd = model.train(x_train, y_train, epochs=30, batch_size=len(x_train), x_test=x_test, y_test=y_test)\n",
    "        # Mini-Batch Gradient Descent\n",
    "        layers = [Linear(784, 50), ReLU(), Linear(50, 50), ReLU(), Linear(50, len(kept_classes)), Softmax()]\n",
    "        model = NN(layers, CrossEntropy(), lr=0.001)\n",
    "        print('Stochastic Gradient Descent:', end='\\n\\t')\n",
    "        sgd = model.train(x_train, y_train, epochs=30, batch_size=1, x_test=x_test, y_test=y_test)\n",
    "        first_run = False\n",
    "    else:\n",
    "        print('\\n')\n",
    "    # Stochastic Gradient Descent\n",
    "    layers = [Linear(784, 50), ReLU(), Linear(50, 50), ReLU(), Linear(50, len(kept_classes)), Softmax()]\n",
    "    model = NN(layers, CrossEntropy(), lr=0.001)\n",
    "    print('Mini-Batch Gradient Descent:', end='\\n\\t')\n",
    "    mbgd = model.train(x_train, y_train, epochs=30, batch_size=mini_batch_size, x_test=x_test, y_test=y_test)\n",
    "    print()\n",
    "    plot_batch_size(gd, sgd, mbgd)\n",
    "\n",
    "\n",
    "\n",
    "# Observe changes in the dropdown value\n",
    "dropdown.observe(on_value_change, names='value')\n",
    "\n",
    "# Run Vanila Gradient Descent and Stochastic Gradient Descent once\n",
    "first_run = True\n",
    "\n",
    "gd, sgd, mbgd = None, None, None\n",
    "\n",
    "# Display the widget\n",
    "display(dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpnbFt4zkgu9"
   },
   "source": [
    "As you can see, increasing the batch size improves the speed of our algorithm, while reducing the batch size allows us to achieve higher accuracies."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Ecaqr9YkWpbL"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1179f322ebf643fcbc389efa1e80f0d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DropdownModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DropdownModel",
      "_options_labels": [
       "16",
       "32",
       "64",
       "128",
       "256"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "DropdownView",
      "description": "Batch Size:",
      "description_tooltip": null,
      "disabled": false,
      "index": 4,
      "layout": "IPY_MODEL_b9ab8195f2d94405a822ea67ea1fd094",
      "style": "IPY_MODEL_48a192d8f8754cac918a4ea1fa60e23a"
     }
    },
    "48a192d8f8754cac918a4ea1fa60e23a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b9ab8195f2d94405a822ea67ea1fd094": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "200px"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
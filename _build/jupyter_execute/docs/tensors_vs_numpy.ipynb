{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9371da34",
   "metadata": {},
   "source": [
    "# Tensors in PyTorch: What Changes Compared to the From-Scratch Implementation?\n",
    "\n",
    "In the previous section, we implemented a Multilayer Perceptron (MLP) **from scratch** using basic Python and NumPy arrays. \n",
    "All computations were expressed in terms of scalars, vectors, and matrices, and we explicitly managed:\n",
    "\n",
    "- the forward pass,\n",
    "- gradient derivations and the backward pass,\n",
    "- parameter updates.\n",
    "\n",
    "PyTorch introduces a new core data type: the **tensor**. While tensors may look similar to NumPy arrays, they add capabilities that are central to modern deep learning systems: **automatic differentiation**, **hardware acceleration**, and a library of optimized deep learning operators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b261feff",
   "metadata": {},
   "source": [
    "## 1. What Is a Tensor?\n",
    "\n",
    "In numerical computing, a **tensor** is a multi-dimensional array. The term emphasizes that we may work with data of arbitrary order (number of axes).\n",
    "\n",
    "- Scalars are **0D tensors**\n",
    "- Vectors are **1D tensors**\n",
    "- Matrices are **2D tensors**\n",
    "- Higher-dimensional arrays are **3D+ tensors**\n",
    "\n",
    "Mathematically, one can view a tensor of order $k$ as an element of a tensor product space:\n",
    "$$\n",
    "\\mathbf{T} \\in V_1 \\otimes V_2 \\otimes \\cdots \\otimes V_k.\n",
    "$$\n",
    "\n",
    "In the MLP context, tensors represent the same objects you used earlier—only the container and execution model change.\n",
    "\n",
    "| Mathematical object | From-scratch code | PyTorch |\n",
    "|---|---|---|\n",
    "| Scalar | `float` | 0D tensor |\n",
    "| Vector | 1D NumPy array | 1D tensor |\n",
    "| Matrix | 2D NumPy array | 2D tensor |\n",
    "| Batch of matrices | 3D array | 3D tensor |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecc34d5",
   "metadata": {},
   "source": [
    "## 2. Why Not Just Use NumPy Arrays? (A More Convincing Answer)\n",
    "\n",
    "NumPy arrays are excellent **numerical containers** and are sufficient for forward computation. However, deep learning workloads require additional *system-level guarantees* and *capabilities* that NumPy does not provide out of the box:\n",
    "\n",
    "1. **Automatic differentiation (autograd)**\n",
    "   - Deep networks require gradients such as $\\nabla_\\theta L(\\theta)$ for millions of parameters $\\theta$.\n",
    "   - With NumPy, gradients must be derived and coded manually or via external tools.\n",
    "\n",
    "2. **Hardware acceleration and device abstraction**\n",
    "   - Training modern models efficiently depends on GPUs (and sometimes other accelerators).\n",
    "   - NumPy operations run on CPU. GPU support requires switching libraries (e.g., CuPy) and re-auditing the pipeline.\n",
    "\n",
    "3. **A differentiable operator ecosystem**\n",
    "   - Deep learning uses specialized ops (convolutions, normalization, embedding lookups, fused kernels).\n",
    "   - PyTorch provides these operators **together with** correct gradient rules and optimized kernels.\n",
    "\n",
    "A useful summary is:\n",
    "- **NumPy**: array computing (values only)\n",
    "- **PyTorch tensor**: array computing **plus** gradient tracking **plus** device-aware execution **plus** deep-learning primitives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9421098b",
   "metadata": {},
   "source": [
    "## 3. Side-by-Side: NumPy Arrays vs PyTorch Tensors (Values)\n",
    "\n",
    "Consider a linear layer (affine map):\n",
    "$$\n",
    "\\mathbf{Y} = \\mathbf{X}\\mathbf{W} + \\mathbf{b},\n",
    "$$\n",
    "with $\\mathbf{X} \\in \\mathbb{R}^{N \\times d}$, $\\mathbf{W} \\in \\mathbb{R}^{d \\times m}$, and $\\mathbf{b} \\in \\mathbb{R}^{m}$.\n",
    "\n",
    "Both NumPy and PyTorch can compute $\\mathbf{Y}$ as a forward pass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f79e70f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.2955051 , -0.70672864],\n",
       "       [ 1.38728186,  0.24221777],\n",
       "       [ 0.8147218 , -0.76782153],\n",
       "       [ 2.86228391, -1.05442875]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NumPy: forward computation (values only)\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(4, 3)      # N=4, d=3\n",
    "W = np.random.randn(3, 2)      # d=3, m=2\n",
    "b = np.random.randn(2,)        # m=2\n",
    "\n",
    "Y_np = X @ W + b\n",
    "Y_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f510a8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6639, -0.6620],\n",
       "        [ 0.5748, -1.5384],\n",
       "        [-1.7279, -1.2307],\n",
       "        [-0.0104, -1.9583]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch: forward computation (values only)\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "X_t = torch.randn(4, 3)\n",
    "W_t = torch.randn(3, 2)\n",
    "b_t = torch.randn(2)\n",
    "\n",
    "Y_t = X_t @ W_t + b_t\n",
    "Y_t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51da0dc5",
   "metadata": {},
   "source": [
    "At this point, the two libraries look similar. The crucial differences appear when we need **gradients**, **devices**, and **training loops**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2d87a6",
   "metadata": {},
   "source": [
    "## 4. Tensors and Automatic Differentiation (Computation Graphs)\n",
    "\n",
    "In gradient-based learning, we minimize a loss $L(\\theta)$ over parameters $\\theta$ (weights and biases). Training requires:\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta L(\\theta),\n",
    "$$\n",
    "where $\\eta$ is the learning rate.\n",
    "\n",
    "In the from-scratch section, you explicitly coded partial derivatives such as:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}}, \\quad \\frac{\\partial L}{\\partial \\mathbf{b}}.\n",
    "$$\n",
    "\n",
    "PyTorch tensors can **track computation graphs**. If a tensor is created with `requires_grad=True`, PyTorch records the sequence of differentiable operations. Calling `backward()` applies the chain rule automatically.\n",
    "\n",
    "The chain rule in backpropagation has the generic form:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} = \\frac{\\partial L}{\\partial \\mathbf{Y}}\\,\\frac{\\partial \\mathbf{Y}}{\\partial \\mathbf{W}}.\n",
    "$$\n",
    "\n",
    "For $\\mathbf{Y}=\\mathbf{X}\\mathbf{W}+\\mathbf{b}$, this becomes:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} = \\mathbf{X}^\\top\\frac{\\partial L}{\\partial \\mathbf{Y}}, \n",
    "\\qquad\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}} = \\sum_{i=1}^{N} \\frac{\\partial L}{\\partial \\mathbf{Y}_{i,:}}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73bec07",
   "metadata": {},
   "source": [
    "### Side-by-Side: Manual Gradients (NumPy) vs Autograd (PyTorch)\n",
    "\n",
    "We will use a simple scalar loss:\n",
    "$$\n",
    "L = \\sum_{i,j} Y_{ij}.\n",
    "$$\n",
    "\n",
    "Then $\\frac{\\partial L}{\\partial Y_{ij}} = 1$ for all entries, so $\\frac{\\partial L}{\\partial \\mathbf{Y}}$ is a matrix of ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7fded36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[5.36563246, 5.36563246],\n",
       "        [2.26040156, 2.26040156],\n",
       "        [1.35251476, 1.35251476]]),\n",
       " array([4., 4.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NumPy: manual gradients for L = sum(Y)\n",
    "grad_Y = np.ones_like(Y_np)          # dL/dY\n",
    "grad_W = X.T @ grad_Y                # dL/dW = X^T dL/dY\n",
    "grad_b = grad_Y.sum(axis=0)          # dL/db = sum over batch\n",
    "\n",
    "grad_W, grad_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e81c062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.8637, -0.8637],\n",
       "         [ 1.3759,  1.3759],\n",
       "         [ 0.8702,  0.8702]]),\n",
       " tensor([4., 4.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch: autograd for the same computation\n",
    "X_t = torch.randn(4, 3, requires_grad=True)\n",
    "W_t = torch.randn(3, 2, requires_grad=True)\n",
    "b_t = torch.randn(2, requires_grad=True)\n",
    "\n",
    "Y = X_t @ W_t + b_t\n",
    "L = Y.sum()\n",
    "L.backward()\n",
    "\n",
    "W_t.grad, b_t.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdde4cd",
   "metadata": {},
   "source": [
    "**Key takeaway:** Autograd does not change the mathematics of backpropagation; it changes *who writes* the gradient code. \n",
    "You still conceptually start from the loss and propagate backward—PyTorch simply performs the bookkeeping consistently and efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9944ba",
   "metadata": {},
   "source": [
    "## 5. Tensor Data Types (`dtype`) and Why They Matter\n",
    "\n",
    "Every tensor has a `dtype` that controls numerical precision and valid operations. Common choices include:\n",
    "\n",
    "- `torch.float32` (default for neural network weights and activations)\n",
    "- `torch.float64` (higher precision; typically slower and rarely needed for standard training)\n",
    "- `torch.int64` (commonly used for class labels)\n",
    "\n",
    "This becomes important in classification. For example, `CrossEntropyLoss` expects labels as integer class indices:\n",
    "$$\n",
    "y \\in \\{0, 1, \\dots, C-1\\},\n",
    "$$\n",
    "not one-hot vectors.\n",
    "\n",
    "In the MNIST workflow:\n",
    "- Inputs `x` are floating-point tensors (e.g., `float32`)\n",
    "- Labels `y` are integer tensors (typically `int64`)\n",
    "\n",
    "NumPy will often silently cast types in mixed operations, which can hide bugs. PyTorch is stricter in many training-critical paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "832354d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dtype illustration\n",
    "x = torch.randn(2, 3)          # float32 by default\n",
    "y = torch.tensor([1, 0])       # int64 by default for integer literals\n",
    "\n",
    "x.dtype, y.dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9221c688",
   "metadata": {},
   "source": [
    "## 6. Tensor Shape and Batching\n",
    "\n",
    "A major practical difference between educational “from-scratch” code and production deep learning code is **batching**.\n",
    "\n",
    "For MNIST, a batch of images typically has shape:\n",
    "$$\n",
    "(\\text{batch}, \\text{channels}, \\text{height}, \\text{width}) = (B, 1, 28, 28).\n",
    "$$\n",
    "\n",
    "An MLP expects a matrix of shape $(B, 784)$, so we reshape (flatten) each image:\n",
    "$$\n",
    "\\mathbf{X} \\in \\mathbb{R}^{B \\times 784}.\n",
    "$$\n",
    "\n",
    "In PyTorch, flattening is often written as:\n",
    "`x = x.view(x.size(0), -1)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa2a4504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 1, 28, 28]), torch.Size([128, 784]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape and flattening example\n",
    "B = 128\n",
    "x_batch = torch.randn(B, 1, 28, 28)\n",
    "x_flat = x_batch.view(x_batch.size(0), -1)\n",
    "\n",
    "x_batch.shape, x_flat.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b306da",
   "metadata": {},
   "source": [
    "## 7. Device Awareness (CPU vs GPU)\n",
    "\n",
    "PyTorch tensors are **device-aware**: each tensor lives on a specific device (CPU or GPU). \n",
    "The same code can run on a GPU by moving tensors and models to that device:\n",
    "\n",
    "```python\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x = x.to(device)\n",
    "model = model.to(device)\n",
    "```\n",
    "\n",
    "NumPy arrays do not have this concept. To use a GPU in a NumPy-like workflow, you must typically switch libraries (and sometimes APIs), which increases complexity and maintenance cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cdd1bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'), device(type='cpu'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device illustration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x = torch.randn(3, 4)\n",
    "x_device = x.to(device)\n",
    "\n",
    "x.device, x_device.device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce3b744",
   "metadata": {},
   "source": [
    "## 8. Summary: Connecting Both Worlds\n",
    "\n",
    "- The **mathematics** of the MLP is identical in both approaches.\n",
    "- The **from-scratch** implementation emphasizes understanding:\n",
    "  - explicit forward/backward derivations,\n",
    "  - explicit parameter updates.\n",
    "- PyTorch tensors emphasize scalability and correctness:\n",
    "  - automatic differentiation,\n",
    "  - standardized batching,\n",
    "  - device-aware execution,\n",
    "  - and a large library of optimized differentiable operators.\n",
    "\n",
    "Learning tensors effectively does not replace understanding backpropagation—it *operationalizes* it for real training workloads.\n",
    "\n",
    "At a high level, a PyTorch tensor is indeed a multi-dimensional array, similar in structure to a NumPy array. If we restrict attention only to numerical storage and basic linear algebra on the CPU, then NumPy and PyTorch tensors may appear interchangeable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4866b7e-acb4-44e5-beb8-04a81c7efe6b",
   "metadata": {},
   "source": [
    "# 9. How Does a Tensor Store Information for Differentiation?\n",
    "\n",
    "In PyTorch, a tensor is **not just a numerical array**. In addition to storing values, a tensor carries **metadata** that enables *automatic differentiation* (autograd).  \n",
    "This section explains what information is stored, where it is stored, and how it is used during backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2047b68b-e2f9-486a-9539-7bf8d6a42f01",
   "metadata": {},
   "source": [
    "## 10. Conceptual Structure of a Tensor\n",
    "\n",
    "A tensor participating in differentiation can be abstracted as:\n",
    "\n",
    "$$\n",
    "\\text{Tensor} = (\\text{data}, \\text{requires\\_grad}, \\text{grad}, \\text{grad\\_fn})\n",
    "$$\n",
    "\n",
    "- **data**: numerical values in CPU or GPU memory  \n",
    "- **requires_grad**: whether gradients should be tracked  \n",
    "- **grad**: stores $\\frac{\\partial L}{\\partial \\text{tensor}}$ after backpropagation  \n",
    "- **grad_fn**: reference to the operation that created the tensor  \n",
    "\n",
    "This metadata differentiates PyTorch tensors from NumPy arrays.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0224ff86-5ece-424b-a5ca-59e35fbf10d6",
   "metadata": {},
   "source": [
    "## 11. Leaf Tensors vs Non-Leaf Tensors\n",
    "\n",
    "### Leaf tensors\n",
    "- Created directly by the user  \n",
    "- Have `requires_grad=True`  \n",
    "- Store gradients in `.grad`  \n",
    "\n",
    "Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83e668f3-d377-4fc9-b7a9-d54c5e85ccd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "X = torch.randn(4, 3)\n",
    "W = torch.randn(3, 2, requires_grad=True)\n",
    "b = torch.randn(2, requires_grad=True)\n",
    "\n",
    "Y = X @ W + b\n",
    "L = Y.sum()\n",
    "L.backward()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845ea133-d0fd-4c75-9b5e-4d800aab55dd",
   "metadata": {},
   "source": [
    "### Non-leaf tensors\n",
    "- Results of operations  \n",
    "- Possess a `grad_fn`  \n",
    "- Do not store gradients by default  \n",
    "\n",
    "Intermediate activations in neural networks are typically non-leaf tensors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc370bc-2916-4b9b-a218-ef833874d44c",
   "metadata": {},
   "source": [
    "## 12. Computation Graph\n",
    "\n",
    "Each tensor operation adds a node to a **directed acyclic graph (DAG)**:\n",
    "\n",
    "$$\n",
    "\\text{inputs} \\rightarrow \\text{operation} \\rightarrow \\text{output}\n",
    "$$\n",
    "\n",
    "For:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9b1aec4-2cdb-4236-ad9c-87ff4a10cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Y = X @ W + b\n",
    "L = Y.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e21437b-dcc8-43d0-9634-9b81c1878141",
   "metadata": {},
   "source": [
    "\n",
    "The graph conceptually follows:\n",
    "\n",
    "```\n",
    "X ----\\\n",
    "       MatMul ---- Add ---- Sum ---- L\n",
    "W ----/              ^\n",
    "                     |\n",
    "                     b\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19b4691-c162-4204-b1f5-1b59a9276a77",
   "metadata": {},
   "source": [
    "## 13. Backward Functions (`grad_fn`)\n",
    "\n",
    "The attribute `grad_fn` references a backward-function object generated by the forward operation.\n",
    "\n",
    "For example:\n",
    "\n",
    "\n",
    "This object encodes how to compute:\n",
    "$$\n",
    "\\frac{\\partial Y}{\\partial X}, \\quad \\frac{\\partial Y}{\\partial W}.\n",
    "$$\n",
    "\n",
    "Thus, each forward operation implicitly defines its backward rule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98357c73-535e-4c6e-a645-a77fde4b6778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MmBackward0 object at 0x1084b7b50>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Y = X @ W\n",
    "print(Y.grad_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3965dd12-b9aa-4889-8b3b-5f60e9d8bed0",
   "metadata": {},
   "source": [
    "## 5. Backward Pass and the Chain Rule\n",
    "\n",
    "Calling:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc2bee25-6d5e-433f-9549-6a332fe437ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "L.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a1983b-ab51-46b9-a838-e8a2304d80b0",
   "metadata": {},
   "source": [
    "initiates reverse-mode automatic differentiation:\n",
    "\n",
    "1. Initialize $\\frac{\\partial L}{\\partial L} = 1$  \n",
    "2. Traverse the computation graph in reverse  \n",
    "3. Apply the chain rule:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x}\n",
    "=\n",
    "\\frac{\\partial L}{\\partial y}\n",
    "\\frac{\\partial y}{\\partial x}\n",
    "$$\n",
    "4. Accumulate gradients for leaf tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee89dca9-88ba-4029-bc11-8915b0724294",
   "metadata": {},
   "source": [
    "## 14. Gradient Accumulation\n",
    "\n",
    "Gradients accumulate by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8340639b-b74c-427f-b76f-2a19e592417d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m L\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniforge3/envs/jupyterbook/lib/python3.10/site-packages/torch/_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    624\u001b[0m     )\n\u001b[0;32m--> 625\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/jupyterbook/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/jupyterbook/lib/python3.10/site-packages/torch/autograd/graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "\n",
    "L.backward()\n",
    "L.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee04299-04c0-4fb1-8300-951a437f226b",
   "metadata": {},
   "source": [
    "yields:\n",
    "$$\n",
    "\\text{grad} = \\frac{\\partial L}{\\partial \\theta} + \\frac{\\partial L}{\\partial \\theta}.\n",
    "$$\n",
    "\n",
    "This supports mini-batch training and necessitates clearing gradients explicitly with `optimizer.zero_grad()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c797bb31-1172-4ae2-9121-c35939a26ca4",
   "metadata": {},
   "source": [
    "## 15. Why NumPy Arrays Cannot Do This\n",
    "\n",
    "NumPy arrays:\n",
    "- store only values  \n",
    "- do not record operation history  \n",
    "- have no backward rules  \n",
    "- lack gradient storage  \n",
    "\n",
    "Therefore, gradient-based learning in NumPy requires manual implementation of backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be33ce6-0362-4111-93b5-4c226d37ff99",
   "metadata": {},
   "source": [
    "## 16. Disabling Autograd\n",
    "\n",
    "To avoid graph construction during inference:\n",
    "```pyhton\n",
    "with torch.no_grad():\n",
    "    y = model(x)\n",
    "````\n",
    "\n",
    "This makes tensors behave more like NumPy arrays while preserving API consistency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d34811-c0aa-4d62-89e9-829a82511892",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- PyTorch tensors extend arrays with differentiation metadata  \n",
    "- Computation graphs are built dynamically  \n",
    "- Gradients are computed by reverse traversal using the chain rule  \n",
    "- Autograd scales manual backpropagation reliably to large models  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
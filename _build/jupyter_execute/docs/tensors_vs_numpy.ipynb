{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9371da34",
   "metadata": {},
   "source": [
    "# Tensors in PyTorch: What Changes Compared to the From-Scratch Implementation?\n",
    "\n",
    "In the previous section, we implemented a Multilayer Perceptron (MLP) **from scratch** using basic Python and NumPy arrays. \n",
    "All computations were expressed in terms of scalars, vectors, and matrices, and we explicitly managed:\n",
    "\n",
    "- the forward pass,\n",
    "- gradient derivations and the backward pass,\n",
    "- parameter updates.\n",
    "\n",
    "PyTorch introduces a new core data type: the **tensor**. While tensors may look similar to NumPy arrays, they add capabilities that are central to modern deep learning systems: **automatic differentiation**, **hardware acceleration**, and a library of optimized deep learning operators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b261feff",
   "metadata": {},
   "source": [
    "## 1. What Is a Tensor?\n",
    "\n",
    "In numerical computing, a **tensor** is a multi-dimensional array. The term emphasizes that we may work with data of arbitrary order (number of axes).\n",
    "\n",
    "- Scalars are **0D tensors**\n",
    "- Vectors are **1D tensors**\n",
    "- Matrices are **2D tensors**\n",
    "- Higher-dimensional arrays are **3D+ tensors**\n",
    "\n",
    "Mathematically, one can view a tensor of order $k$ as an element of a tensor product space:\n",
    "$$\n",
    "\\mathbf{T} \\in V_1 \\otimes V_2 \\otimes \\cdots \\otimes V_k.\n",
    "$$\n",
    "\n",
    "In the MLP context, tensors represent the same objects you used earlier—only the container and execution model change.\n",
    "\n",
    "| Mathematical object | From-scratch code | PyTorch |\n",
    "|---|---|---|\n",
    "| Scalar | `float` | 0D tensor |\n",
    "| Vector | 1D NumPy array | 1D tensor |\n",
    "| Matrix | 2D NumPy array | 2D tensor |\n",
    "| Batch of matrices | 3D array | 3D tensor |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecc34d5",
   "metadata": {},
   "source": [
    "## 2. Why Not Just Use NumPy Arrays? (A More Convincing Answer)\n",
    "\n",
    "NumPy arrays are excellent **numerical containers** and are sufficient for forward computation. However, deep learning workloads require additional *system-level guarantees* and *capabilities* that NumPy does not provide out of the box:\n",
    "\n",
    "1. **Automatic differentiation (autograd)**\n",
    "   - Deep networks require gradients such as $\\nabla_\\theta L(\\theta)$ for millions of parameters $\\theta$.\n",
    "   - With NumPy, gradients must be derived and coded manually or via external tools.\n",
    "\n",
    "2. **Hardware acceleration and device abstraction**\n",
    "   - Training modern models efficiently depends on GPUs (and sometimes other accelerators).\n",
    "   - NumPy operations run on CPU. GPU support requires switching libraries (e.g., CuPy) and re-auditing the pipeline.\n",
    "\n",
    "3. **A differentiable operator ecosystem**\n",
    "   - Deep learning uses specialized ops (convolutions, normalization, embedding lookups, fused kernels).\n",
    "   - PyTorch provides these operators **together with** correct gradient rules and optimized kernels.\n",
    "\n",
    "A useful summary is:\n",
    "- **NumPy**: array computing (values only)\n",
    "- **PyTorch tensor**: array computing **plus** gradient tracking **plus** device-aware execution **plus** deep-learning primitives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9421098b",
   "metadata": {},
   "source": [
    "## 3. Side-by-Side: NumPy Arrays vs PyTorch Tensors (Values)\n",
    "\n",
    "Consider a linear layer (affine map):\n",
    "$$\n",
    "\\mathbf{Y} = \\mathbf{X}\\mathbf{W} + \\mathbf{b},\n",
    "$$\n",
    "with $\\mathbf{X} \\in \\mathbb{R}^{N \\times d}$, $\\mathbf{W} \\in \\mathbb{R}^{d \\times m}$, and $\\mathbf{b} \\in \\mathbb{R}^{m}$.\n",
    "\n",
    "Both NumPy and PyTorch can compute $\\mathbf{Y}$ as a forward pass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f79e70f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.2955051 , -0.70672864],\n",
       "       [ 1.38728186,  0.24221777],\n",
       "       [ 0.8147218 , -0.76782153],\n",
       "       [ 2.86228391, -1.05442875]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NumPy: forward computation (values only)\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(4, 3)      # N=4, d=3\n",
    "W = np.random.randn(3, 2)      # d=3, m=2\n",
    "b = np.random.randn(2,)        # m=2\n",
    "\n",
    "Y_np = X @ W + b\n",
    "Y_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f510a8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6639, -0.6620],\n",
       "        [ 0.5748, -1.5384],\n",
       "        [-1.7279, -1.2307],\n",
       "        [-0.0104, -1.9583]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch: forward computation (values only)\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "X_t = torch.randn(4, 3)\n",
    "W_t = torch.randn(3, 2)\n",
    "b_t = torch.randn(2)\n",
    "\n",
    "Y_t = X_t @ W_t + b_t\n",
    "Y_t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51da0dc5",
   "metadata": {},
   "source": [
    "At this point, the two libraries look similar. The crucial differences appear when we need **gradients**, **devices**, and **training loops**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2d87a6",
   "metadata": {},
   "source": [
    "## 4. Tensors and Automatic Differentiation (Computation Graphs)\n",
    "\n",
    "In gradient-based learning, we minimize a loss $L(\\theta)$ over parameters $\\theta$ (weights and biases). Training requires:\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta L(\\theta),\n",
    "$$\n",
    "where $\\eta$ is the learning rate.\n",
    "\n",
    "In the from-scratch section, you explicitly coded partial derivatives such as:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}}, \\quad \\frac{\\partial L}{\\partial \\mathbf{b}}.\n",
    "$$\n",
    "\n",
    "PyTorch tensors can **track computation graphs**. If a tensor is created with `requires_grad=True`, PyTorch records the sequence of differentiable operations. Calling `backward()` applies the chain rule automatically.\n",
    "\n",
    "The chain rule in backpropagation has the generic form:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} = \\frac{\\partial L}{\\partial \\mathbf{Y}}\\,\\frac{\\partial \\mathbf{Y}}{\\partial \\mathbf{W}}.\n",
    "$$\n",
    "\n",
    "For $\\mathbf{Y}=\\mathbf{X}\\mathbf{W}+\\mathbf{b}$, this becomes:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} = \\mathbf{X}^\\top\\frac{\\partial L}{\\partial \\mathbf{Y}}, \n",
    "\\qquad\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}} = \\sum_{i=1}^{N} \\frac{\\partial L}{\\partial \\mathbf{Y}_{i,:}}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73bec07",
   "metadata": {},
   "source": [
    "### Side-by-Side: Manual Gradients (NumPy) vs Autograd (PyTorch)\n",
    "\n",
    "We will use a simple scalar loss:\n",
    "$$\n",
    "L = \\sum_{i,j} Y_{ij}.\n",
    "$$\n",
    "\n",
    "Then $\\frac{\\partial L}{\\partial Y_{ij}} = 1$ for all entries, so $\\frac{\\partial L}{\\partial \\mathbf{Y}}$ is a matrix of ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7fded36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[5.36563246, 5.36563246],\n",
       "        [2.26040156, 2.26040156],\n",
       "        [1.35251476, 1.35251476]]),\n",
       " array([4., 4.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NumPy: manual gradients for L = sum(Y)\n",
    "grad_Y = np.ones_like(Y_np)          # dL/dY\n",
    "grad_W = X.T @ grad_Y                # dL/dW = X^T dL/dY\n",
    "grad_b = grad_Y.sum(axis=0)          # dL/db = sum over batch\n",
    "\n",
    "grad_W, grad_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e81c062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.8637, -0.8637],\n",
       "         [ 1.3759,  1.3759],\n",
       "         [ 0.8702,  0.8702]]),\n",
       " tensor([4., 4.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch: autograd for the same computation\n",
    "X_t = torch.randn(4, 3, requires_grad=True)\n",
    "W_t = torch.randn(3, 2, requires_grad=True)\n",
    "b_t = torch.randn(2, requires_grad=True)\n",
    "\n",
    "Y = X_t @ W_t + b_t\n",
    "L = Y.sum()\n",
    "L.backward()\n",
    "\n",
    "W_t.grad, b_t.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdde4cd",
   "metadata": {},
   "source": [
    "**Key takeaway:** Autograd does not change the mathematics of backpropagation; it changes *who writes* the gradient code. \n",
    "You still conceptually start from the loss and propagate backward—PyTorch simply performs the bookkeeping consistently and efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9944ba",
   "metadata": {},
   "source": [
    "## 5. Tensor Data Types (`dtype`) and Why They Matter\n",
    "\n",
    "Every tensor has a `dtype` that controls numerical precision and valid operations. Common choices include:\n",
    "\n",
    "- `torch.float32` (default for neural network weights and activations)\n",
    "- `torch.float64` (higher precision; typically slower and rarely needed for standard training)\n",
    "- `torch.int64` (commonly used for class labels)\n",
    "\n",
    "This becomes important in classification. For example, `CrossEntropyLoss` expects labels as integer class indices:\n",
    "$$\n",
    "y \\in \\{0, 1, \\dots, C-1\\},\n",
    "$$\n",
    "not one-hot vectors.\n",
    "\n",
    "In the MNIST workflow:\n",
    "- Inputs `x` are floating-point tensors (e.g., `float32`)\n",
    "- Labels `y` are integer tensors (typically `int64`)\n",
    "\n",
    "NumPy will often silently cast types in mixed operations, which can hide bugs. PyTorch is stricter in many training-critical paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "832354d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dtype illustration\n",
    "x = torch.randn(2, 3)          # float32 by default\n",
    "y = torch.tensor([1, 0])       # int64 by default for integer literals\n",
    "\n",
    "x.dtype, y.dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9221c688",
   "metadata": {},
   "source": [
    "## 6. Tensor Shape and Batching\n",
    "\n",
    "A major practical difference between educational “from-scratch” code and production deep learning code is **batching**.\n",
    "\n",
    "For MNIST, a batch of images typically has shape:\n",
    "$$\n",
    "(\\text{batch}, \\text{channels}, \\text{height}, \\text{width}) = (B, 1, 28, 28).\n",
    "$$\n",
    "\n",
    "An MLP expects a matrix of shape $(B, 784)$, so we reshape (flatten) each image:\n",
    "$$\n",
    "\\mathbf{X} \\in \\mathbb{R}^{B \\times 784}.\n",
    "$$\n",
    "\n",
    "In PyTorch, flattening is often written as:\n",
    "`x = x.view(x.size(0), -1)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa2a4504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 1, 28, 28]), torch.Size([128, 784]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape and flattening example\n",
    "B = 128\n",
    "x_batch = torch.randn(B, 1, 28, 28)\n",
    "x_flat = x_batch.view(x_batch.size(0), -1)\n",
    "\n",
    "x_batch.shape, x_flat.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b306da",
   "metadata": {},
   "source": [
    "## 7. Device Awareness (CPU vs GPU)\n",
    "\n",
    "PyTorch tensors are **device-aware**: each tensor lives on a specific device (CPU or GPU). \n",
    "The same code can run on a GPU by moving tensors and models to that device:\n",
    "\n",
    "```python\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x = x.to(device)\n",
    "model = model.to(device)\n",
    "```\n",
    "\n",
    "NumPy arrays do not have this concept. To use a GPU in a NumPy-like workflow, you must typically switch libraries (and sometimes APIs), which increases complexity and maintenance cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cdd1bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'), device(type='cpu'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device illustration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x = torch.randn(3, 4)\n",
    "x_device = x.to(device)\n",
    "\n",
    "x.device, x_device.device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce3b744",
   "metadata": {},
   "source": [
    "## 8. Summary: Connecting Both Worlds\n",
    "\n",
    "- The **mathematics** of the MLP is identical in both approaches.\n",
    "- The **from-scratch** implementation emphasizes understanding:\n",
    "  - explicit forward/backward derivations,\n",
    "  - explicit parameter updates.\n",
    "- PyTorch tensors emphasize scalability and correctness:\n",
    "  - automatic differentiation,\n",
    "  - standardized batching,\n",
    "  - device-aware execution,\n",
    "  - and a large library of optimized differentiable operators.\n",
    "\n",
    "Learning tensors effectively does not replace understanding backpropagation—it *operationalizes* it for real training workloads.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Machine Learning with PyTorch and Scikit-Learn  \n",
    "# -- Code Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package version checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add folder to path in order to load from the check_packages.py script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check recommended package versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'python_environment_check'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpython_environment_check\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_packages\n\u001b[1;32m      4\u001b[0m d \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1.8.0\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnetworkx\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2.6.2\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1.21.2\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m     10\u001b[0m check_packages(d)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'python_environment_check'"
     ]
    }
   ],
   "source": [
    "from python_environment_check import check_packages\n",
    "\n",
    "\n",
    "d = {\n",
    "    'torch': '1.8.0',\n",
    "    'networkx': '2.6.2',\n",
    "    'numpy': '1.21.2',\n",
    "}\n",
    "\n",
    "check_packages(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 18 - Graph Neural Networks for Capturing Dependencies in Graph Structured Data (Part 1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Introduction to graph data](#Introduction-to-graph-data)\n",
    "  - [Undirected graphs](#Undirected-graphs)\n",
    "  - [Directed graphs](#Directed-graphs)\n",
    "  - [Labeled graphs](#Labeled-graphs)\n",
    "  - [Representing molecules as graphs](#Representing-molecules-as-graphs)\n",
    "- [Understanding graph convolutions](#Understanding-graph-convolutions)\n",
    "  - [The motivation behind using graph convolutions](#The-motivation-behind-using-graph-convolutions)\n",
    "  - [Implementing a basic graph convolution](#Implementing-a-basic-graph-convolution)\n",
    "- [Implementing a GNN in PyTorch from scratch](#Implementing-a-GNN-in-PyTorch-from-scratch)\n",
    "  - [Defining the NodeNetwork model](#Defining-the-NodeNetwork-model)\n",
    "  - [Coding the NodeNetwork’s graph convolution layer](#Coding-the-NodeNetworks-graph-convolution-layer)\n",
    "  - [Adding a global pooling layer to deal with varying graph sizes](#Adding-a-global-pooling-layer-to-deal-with-varying-graph-sizes)\n",
    "  - [Preparing the DataLoader](#Preparing-the-DataLoader)\n",
    "  - [Using the NodeNetwork to make predictions](#Using-the-NodeNetwork-to-make-predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to graph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='figures/18_01.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undirected graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='figures/18_02.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directed graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='figures/18_03.png', width=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeled graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing molecules as graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='figures/18_04.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding graph convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The motivation behind using graph convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='figures/18_05.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a basic graph convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='figures/18_06.png', width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "#Hex codes for colors if we draw graph\n",
    "blue, orange, green = \"#1f77b4\", \"#ff7f0e\",\"#2ca02c\"\n",
    "\n",
    "G.add_nodes_from([(1, {\"color\": blue}),\n",
    "                  (2, {\"color\": orange}),\n",
    "                  (3, {\"color\": blue}),\n",
    "                  (4, {\"color\": green})])\n",
    "\n",
    "G.add_edges_from([(1, 2),(2, 3),(1, 3),(3, 4)])\n",
    "A = np.asarray(nx.adjacency_matrix(G).todense())\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_color_label_representation(G,mapping_dict):\n",
    "    one_hot_idxs = np.array([mapping_dict[v] for v in \n",
    "                             nx.get_node_attributes(G, 'color').values()])\n",
    "    one_hot_encoding = np.zeros((one_hot_idxs.size,len(mapping_dict)))\n",
    "    one_hot_encoding[np.arange(one_hot_idxs.size),one_hot_idxs] = 1\n",
    "    return one_hot_encoding\n",
    "\n",
    "X = build_graph_color_label_representation(G, {green: 0, blue: 1, orange: 2})\n",
    "print(X)\n",
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = nx.get_node_attributes(G, 'color').values()\n",
    "nx.draw(G, with_labels=True, node_color=color_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='figures/18_07.png', width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_in, f_out = X.shape[1], 6\n",
    "W_1 = np.random.rand(f_in, f_out) \n",
    "W_2 = np.random.rand(f_in, f_out) \n",
    "h = np.dot(X,W_1) + np.dot(np.dot(A, X), W_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='figures/18_08.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a GNN in PyTorch from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the NodeNetwork model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeNetwork(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_features):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_1 = BasicGraphConvolutionLayer(input_features, 32)\n",
    "        self.conv_2 = BasicGraphConvolutionLayer(32, 32)\n",
    "        self.fc_1 = torch.nn.Linear(32, 16)\n",
    "        self.out_layer = torch.nn.Linear(16, 2)\n",
    "    \n",
    "    def forward(self, X, A,batch_mat):\n",
    "        x = self.conv_1(X, A).clamp(0)\n",
    "        x = self.conv_2(x, A).clamp(0)\n",
    "        output = global_sum_pool(x, batch_mat)\n",
    "        output = self.fc_1(output)\n",
    "        output = self.out_layer(output)\n",
    "        return F.softmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='figures/18_09.png', width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding the NodeNetwork’s graph convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicGraphConvolutionLayer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.W2 = Parameter(torch.rand(\n",
    "             (in_channels, out_channels), dtype=torch.float32))\n",
    "        self.W1 = Parameter(torch.rand(\n",
    "             (in_channels, out_channels), dtype=torch.float32))\n",
    "         \n",
    "        self.bias = Parameter(torch.zeros(\n",
    "                 out_channels, dtype=torch.float32))\n",
    "    \n",
    "    def forward(self, X, A):\n",
    "        potential_msgs = torch.mm(X, self.W2)\n",
    "        propagated_msgs = torch.mm(A, potential_msgs)\n",
    "        root_update = torch.mm(X, self.W1)\n",
    "        output = propagated_msgs + root_update + self.bias\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a global pooling layer to deal with varying graph sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_sum_pool(X, batch_mat):\n",
    "    if batch_mat is None or batch_mat.dim() == 1:\n",
    "        return torch.sum(X, dim=0).unsqueeze(0)\n",
    "    else:\n",
    "        return torch.mm(batch_mat, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='figures/18_10.png', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_tensor(graph_sizes):\n",
    "    starts = [sum(graph_sizes[:idx]) for idx in range(len(graph_sizes))]\n",
    "    stops = [starts[idx]+graph_sizes[idx] for idx in range(len(graph_sizes))]\n",
    "    tot_len = sum(graph_sizes)\n",
    "    batch_size = len(graph_sizes)\n",
    "    batch_mat = torch.zeros([batch_size, tot_len]).float()\n",
    "    for idx, starts_and_stops in enumerate(zip(starts, stops)):\n",
    "        start = starts_and_stops[0]\n",
    "        stop = starts_and_stops[1]\n",
    "        batch_mat[idx, start:stop] = 1\n",
    "    return batch_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_graphs(batch):\n",
    "    adj_mats = [graph['A'] for graph in batch]\n",
    "    sizes = [A.size(0) for A in adj_mats]\n",
    "    tot_size = sum(sizes)\n",
    "    # create batch matrix\n",
    "    batch_mat = get_batch_tensor(sizes)\n",
    "    # combine feature matrices\n",
    "    feat_mats = torch.cat([graph['X'] for graph in batch],dim=0)\n",
    "    # combine labels\n",
    "    labels = torch.cat([graph['y'] for graph in batch], dim=0)\n",
    "    # combine adjacency matrices\n",
    "    batch_adj = torch.zeros([tot_size, tot_size], dtype=torch.float32)\n",
    "    accum = 0\n",
    "    for adj in adj_mats:\n",
    "        g_size = adj.shape[0]\n",
    "        batch_adj[accum:accum+g_size, accum:accum+g_size] = adj\n",
    "        accum = accum + g_size\n",
    "    repr_and_label = {\n",
    "            'A': batch_adj, \n",
    "            'X': feat_mats,\n",
    "            'y': labels,\n",
    "            'batch' : batch_mat}\n",
    "\n",
    "    return repr_and_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_dict(G, mapping_dict):\n",
    "    # build dictionary representation of graph G\n",
    "    A = torch.from_numpy(np.asarray(nx.adjacency_matrix(G).todense())).float()\n",
    "    # build_graph_color_label_representation() was introduced with the first example graph\n",
    "    X = torch.from_numpy(build_graph_color_label_representation(G,mapping_dict)).float()\n",
    "    # kludge since there is not specific task for this example\n",
    "    y = torch.tensor([[1, 0]]).float()\n",
    "    return {'A': A, 'X': X, 'y': y, 'batch': None}\n",
    "    \n",
    "# building 4 graphs to treat as a dataset\n",
    "\n",
    "blue, orange, green = \"#1f77b4\", \"#ff7f0e\",\"#2ca02c\"\n",
    "mapping_dict = {green: 0, blue: 1, orange: 2}\n",
    "\n",
    "G1 = nx.Graph()\n",
    "G1.add_nodes_from([(1, {\"color\": blue}),\n",
    "                   (2, {\"color\": orange}),\n",
    "                   (3, {\"color\": blue}),\n",
    "                   (4, {\"color\": green})])\n",
    "G1.add_edges_from([(1, 2), (2, 3),(1, 3), (3, 4)])\n",
    "G2 = nx.Graph()\n",
    "G2.add_nodes_from([(1, {\"color\": green}),\n",
    "                   (2, {\"color\": green}),\n",
    "                   (3, {\"color\": orange}),\n",
    "                   (4, {\"color\": orange}),\n",
    "                   (5,{\"color\": blue})])\n",
    "G2.add_edges_from([(2, 3),(3, 4),(3, 1),(5, 1)])\n",
    "G3 = nx.Graph()\n",
    "G3.add_nodes_from([(1, {\"color\": orange}),\n",
    "                   (2, {\"color\": orange}),\n",
    "                   (3, {\"color\": green}),\n",
    "                   (4, {\"color\": green}),\n",
    "                   (5, {\"color\": blue}),\n",
    "                   (6, {\"color\":orange})])\n",
    "G3.add_edges_from([(2, 3), (3, 4), (3, 1), (5, 1), (2, 5), (6, 1)])\n",
    "G4 = nx.Graph()\n",
    "G4.add_nodes_from([(1, {\"color\": blue}), (2, {\"color\": blue}), (3, {\"color\": green})])\n",
    "G4.add_edges_from([(1, 2), (2, 3)])\n",
    "graph_list = [get_graph_dict(graph,mapping_dict) for graph in [G1, G2, G3, G4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='figures/18_11.png', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class ExampleDataset(Dataset):\n",
    "    \n",
    "    # Simple PyTorch dataset that will use our list of graphs\n",
    "    def __init__(self, graph_list):\n",
    "        self.graphs = graph_list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        mol_rep = self.graphs[idx]\n",
    "        return mol_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = ExampleDataset(graph_list)\n",
    "# Note how we use our custom collate function\n",
    "loader = DataLoader(dset, batch_size=2, shuffle=False, collate_fn=collate_graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the NodeNetwork to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "node_features = 3\n",
    "net = NodeNetwork(node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_results = []\n",
    "\n",
    "for b in loader:\n",
    "    batch_results.append(net(b['X'], b['A'], b['batch']).detach())\n",
    "\n",
    "G1_rep = dset[1]\n",
    "G1_single = net(G1_rep['X'], G1_rep['A'], G1_rep['batch']).detach()\n",
    "\n",
    "G1_batch = batch_results[0][1]\n",
    "torch.all(torch.isclose(G1_single, G1_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Node classification in citation network\n",
    "\n",
    "The Cora dataset is a benchmark citation network widely used in graph machine learning and graph neural network research. It consists of 2,708 nodes representing scientific publications and 5,429 edges denoting citation relationships between papers. Each node is described by a 1,433-dimensional binary feature vector indicating the presence or absence of specific words in the paper, and each node belongs to one of seven research topic classes. The dataset is provided as a single, fixed graph with predefined training, validation, and test splits, making it especially suitable for semi-supervised node classification tasks. Cora is commonly used to evaluate algorithms such as Graph Convolutional Networks (GCNs) due to its moderate size, sparse structure, and well-understood characteristics. In the PyTorch Geometric Planetoid version of the Cora dataset, node features are provided as indexed binary vectors without explicit names, although each feature actually corresponds to the presence of a specific word from the original paper vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "cora_dataset = Planetoid(root=\"Cora_data\", name=\"Cora\", transform=NormalizeFeatures())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cora_dataset))\n",
    "data = cora_dataset[0]\n",
    "print(cora_dataset)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cora Dataset Inspection (PyTorch Geometric) =====\n",
    "\n",
    "print(\"===== GRAPH SUMMARY =====\")\n",
    "print(data)\n",
    "print()\n",
    "\n",
    "print(\"===== BASIC STATISTICS =====\")\n",
    "print(f\"Number of nodes: {data.num_nodes}\")\n",
    "print(f\"Number of edges: {data.num_edges}\")\n",
    "print(f\"Average node degree: {data.num_edges / data.num_nodes:.2f}\")\n",
    "print(f\"Is undirected: {data.is_undirected()}\")\n",
    "print(f\"Number of Feature: {data.num_features}\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"===== NODE FEATURES =====\")\n",
    "print(f\"Feature matrix shape: {data.x.shape}\")\n",
    "print(f\"Number of node features: {cora_dataset.num_node_features}\")\n",
    "print(f\"First node feature vector:\\n{data.x[0]}\")\n",
    "print()\n",
    "\n",
    "print(\"===== LABELS =====\")\n",
    "print(f\"Label tensor shape: {data.y.shape}\")\n",
    "print(f\"Number of classes: {cora_dataset.num_classes}\")\n",
    "print()\n",
    "\n",
    "print(\"===== DATA SPLITS =====\")\n",
    "print(f\"Training nodes: {int(data.train_mask.sum())}\")\n",
    "print(f\"Validation nodes: {int(data.val_mask.sum())}\")\n",
    "print(f\"Test nodes: {int(data.test_mask.sum())}\")\n",
    "print()\n",
    "\n",
    "print(\"===== CLASS DISTRIBUTION =====\")\n",
    "for c in range(cora_dataset.num_classes):\n",
    "    count = int((data.y == c).sum())\n",
    "    print(f\"Class {c}: {count} nodes\")\n",
    "print()\n",
    "\n",
    "print(\"===== EDGE INFORMATION =====\")\n",
    "print(f\"Edge index shape: {data.edge_index.shape}\")\n",
    "print(\"First 10 edges:\")\n",
    "print(data.edge_index[:, :10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual label mapping based on original Cora dataset documentation\n",
    " Note: PyTorch Geometric does not expose the official label-name mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_dict = {\n",
    "    0: \"Theory\",\n",
    "    1: \"Reinforcement_Learning\",\n",
    "    2: \"Genetic_Algorithms\",\n",
    "    3: \"Neural_Networks\",\n",
    "    4: \"Probabilistic_Methods\",\n",
    "    5: \"Case_Based\",\n",
    "    6: \"Rule_Learning\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Count class occurrences\n",
    "counter = collections.Counter(data.y.numpy())\n",
    "counter = dict(counter)\n",
    "print(counter)\n",
    "\n",
    "# Sort by class index\n",
    "classes = sorted(counter.keys())\n",
    "counts = [counter[c] for c in classes]\n",
    "labels = [label_dict[c] for c in classes]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(labels, counts)\n",
    "plt.xlabel(\"Class\", fontsize=14)\n",
    "plt.ylabel(\"Number of Nodes\", fontsize=14)\n",
    "plt.xticks(rotation=30)\n",
    "plt.title(\"Class Distribution in Cora Dataset\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "\n",
    "# Color palette (one per class)\n",
    "colorlist = [\n",
    "    \"#1f77b4\", \"#d62728\", \"#4daf4a\",\n",
    "    \"#984ea3\", \"#ff7f00\", \"#ffff33\", \"#8c564b\"\n",
    "]\n",
    "\n",
    "# Convert PyG graph to NetworkX graph\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "# Assign node colors based on labels\n",
    "node_colors = [colorlist[int(label)] for label in data.y]\n",
    "\n",
    "# Layout\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "\n",
    "\n",
    "# Prepare node colors\n",
    "node_colors = [colorlist[int(label)] for label in data.y]\n",
    "\n",
    "# Layout\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "nx.draw_networkx_nodes(G, pos, node_size=5, node_color=node_colors)\n",
    "nx.draw_networkx_edges(G, pos, width=0.25)\n",
    "\n",
    "# Legend (explicit index-based mapping)\n",
    "for i in range(len(label_dict)):\n",
    "    plt.scatter([], [], c=colorlist[i], label=label_dict[i])\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc=\"upper left\")\n",
    "plt.title(\"Cora Citation Network (Colored by Class)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-layer Graph Convolutional Network (GCN) \n",
    "\n",
    "The follwoing two-layer Graph Convolutional Network (GCN) takes node features and graph connectivity as input, learns hidden node representations through neighborhood aggregation, and outputs class logits for each node, where the number of output channels corresponds to the number of target classes in the dataset.\n",
    "The GCN class defines a simple two-layer Graph Convolutional Network used for node classification on graph-structured data such as the Cora citation network. The `__init__` method initializes the model by setting a fixed random seed for reproducibility and defining two graph convolution layers: the first layer (`conv1`) transforms the input node features into a lower-dimensional hidden representation, while the second layer (`conv2`) maps these hidden representations to the final output space, whose size equals the number of target classes. In the forward method, the model receives a graph object containing node features (`x`) and edge information (`edge_index`), applies the first graph convolution followed by a ReLU activation to introduce non-linearity, and then uses dropout to reduce overfitting during training. Finally, the second graph convolution produces a matrix of output scores (`logits`) for each node, which are later used with a loss function such as cross-entropy to train the model for node classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, hidden_channels=16):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(123)\n",
    "\n",
    "        self.conv1 = GCNConv(in_channels=input_channels, out_channels=hidden_channels)\n",
    "        self.conv2 = GCNConv(in_channels=hidden_channels, out_channels=output_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GCN(\n",
    "    input_channels=cora_dataset.num_node_features,\n",
    "    output_channels=cora_dataset.num_classes,\n",
    "    hidden_channels=16\n",
    ")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data)\n",
    "\n",
    "print(\"out:\", out.shape, out.dtype, out.device)\n",
    "print(\"labels:\", data.y.shape, data.y.dtype, data.y.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "\n",
    "def visualize_tsne(emb, labels, seed=42):\n",
    "    X = emb.detach().cpu().numpy().astype(np.float32)\n",
    "    y = labels.detach().cpu().numpy()\n",
    "\n",
    "    n = X.shape[0]\n",
    "    perplexity = min(30, max(5, (n - 1) // 3))\n",
    "\n",
    "    tsne = TSNE(\n",
    "        n_components=2,\n",
    "        init=\"pca\",\n",
    "        learning_rate=\"auto\",\n",
    "        perplexity=perplexity,\n",
    "        random_state=seed,\n",
    "        method=\"barnes_hut\",\n",
    "        angle=0.5\n",
    "    )\n",
    "\n",
    "    Z = tsne.fit_transform(X)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.scatter(Z[:, 0], Z[:, 1], s=15, c=y, cmap=\"Set2\")\n",
    "    plt.title(\"t-SNE of node embeddings\")\n",
    "    plt.show()\n",
    "\n",
    "# Run safely\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data)\n",
    "\n",
    "visualize_tsne(out, data.y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code initializes the main components required to train a Graph Convolutional Network (GCN) for node classification. First, the GCN model is created by specifying the number of input channels (equal to the number of node features) and output channels (equal to the number of classes), and then moved to the chosen computation device (CPU or GPU) using .to(device). Next, the Adam optimizer is defined, which updates the model parameters during training; the learning rate controls how large each update step is, while weight decay acts as a regularization term to reduce overfitting. Finally, the loss function is set to CrossEntropyLoss, which is the standard choice for multi-class classification problems and compares the model’s predicted class scores (logits) with the true node labels. Together, these components prepare the model for the training loop, where the optimizer minimizes the loss by adjusting the GCN’s weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# input_channels = dataset.num_node_features\n",
    "# output_channels = dataset.num_classes\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GCN(\n",
    "    input_channels=cora_dataset.num_node_features,\n",
    "    output_channels=cora_dataset.num_classes,\n",
    "    hidden_channels=16)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.01,\n",
    "    weight_decay=5e-4\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code implements the training loop for a Graph Convolutional Network (GCN) on the Cora dataset. First, the number of training epochs is set, which determines how many times the model will see the entire graph during training. The graph data is then moved to the chosen device (CPU or GPU) so that computations are performed consistently.\n",
    "\n",
    "Inside the loop, `model.train()` puts the network in training mode, enabling behaviors such as dropout. At the start of each epoch, existing gradients are cleared using `optimizer.zero_grad()` to prevent accumulation from previous updates. The model then performs a forward pass on the full graph, producing class scores for every node.\n",
    "\n",
    "The loss is computed only on the training nodes using train_mask, which is important in semi-supervised learning because labels are available for only a subset of nodes. After computing the loss, `loss.backward()` calculates gradients with respect to the model parameters, and `optimizer.step()` updates those parameters to reduce the loss.\n",
    "\n",
    "Finally, the code calculates training accuracy by comparing the predicted class of each training node with its true label. Printing the loss and accuracy every few epochs allows students to monitor whether the model is learning and improving over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "num_epochs = 200\n",
    "\n",
    "# Move graph to device\n",
    "cora_graph = cora_dataset[0]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    out = model(cora_graph)\n",
    "\n",
    "    # Compute loss on training nodes only\n",
    "    loss = criterion(\n",
    "        out[cora_graph.train_mask],\n",
    "        cora_graph.y[cora_graph.train_mask]\n",
    "    )\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Training accuracy\n",
    "    pred_train = out.argmax(dim=1)\n",
    "    correct_train = (\n",
    "        pred_train[cora_graph.train_mask]\n",
    "        == cora_graph.y[cora_graph.train_mask]\n",
    "    ).sum()\n",
    "\n",
    "    acc_train = int(correct_train) / int(cora_graph.train_mask.sum())\n",
    "\n",
    "    # Print progress\n",
    "    if epoch % 10 == 0:\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d} | \"\n",
    "            f\"Loss: {loss.item():.4f} | \"\n",
    "            f\"Train Accuracy: {acc_train:.4f}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code evaluates the trained Graph Convolutional Network on the test portion of the dataset, which contains nodes that were not used during training. The call to `model.eval()` puts the network into evaluation mode, ensuring that layers such as dropout behave correctly and do not introduce randomness during testing.\n",
    "\n",
    "The with `torch.no_grad()`: block tells PyTorch not to compute gradients, which saves memory and speeds up execution since the model parameters are not being updated. The model then produces output scores for each node, and `argmax(dim=1)` selects the most likely class for every node.\n",
    "\n",
    "To fairly measure performance, the code uses the test_mask to compare predictions only on test nodes, ignoring training and validation nodes. The number of correct predictions is divided by the total number of test nodes to compute the test accuracy, which reflects how well the model generalizes to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient computation for efficiency\n",
    "with torch.no_grad():\n",
    "    # Forward pass and predicted class labels\n",
    "    pred = model(data).argmax(dim=1)\n",
    "\n",
    "    # Count correct predictions on test nodes only\n",
    "    correct = (\n",
    "        pred[data.test_mask]\n",
    "        == cora_graph.y[data.test_mask]\n",
    "    ).sum().item()\n",
    "\n",
    "    # Compute test accuracy\n",
    "    test_acc = correct / data.test_mask.sum().item()\n",
    "\n",
    "# Print test accuracy\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create the GCN model\n",
    "model = GCN(\n",
    "    input_channels=cora_dataset.num_node_features,\n",
    "    output_channels=cora_dataset.num_classes,\n",
    ")\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.01,\n",
    "    weight_decay=5e-4\n",
    ")\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code extends the basic training loop by adding validation monitoring and best model selection, which are essential practices in machine learning. During each epoch, the model is first trained using only the nodes indicated by train_mask, and the loss is computed and minimized using backpropagation. Training accuracy is calculated to show how well the model fits the labeled training nodes.\n",
    "\n",
    "After training in each epoch, the model is switched to evaluation mode to compute validation accuracy using val_mask. Validation nodes are not used for learning; instead, they provide an unbiased way to measure how well the model generalizes during training. By comparing validation accuracy across epochs, the code identifies when the model performs best on unseen data.\n",
    "\n",
    "Whenever a new highest validation accuracy is achieved, the current model parameters are saved using a deep copy. After training finishes, the model is restored to this best-performing state. This approach helps prevent overfitting and ensures that the final model used for testing is the one that generalized best, not necessarily the one from the last training epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "# Track best validation accuracy and model\n",
    "best_acc_val = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = model(cora_graph)\n",
    "\n",
    "    loss = criterion(\n",
    "        out[cora_graph.train_mask],\n",
    "        cora_graph.y[cora_graph.train_mask]\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Training accuracy\n",
    "    pred_train = out.argmax(dim=1)\n",
    "    correct_train = (\n",
    "        pred_train[cora_graph.train_mask]\n",
    "        == cora_graph.y[cora_graph.train_mask]\n",
    "    ).sum()\n",
    "    acc_train = int(correct_train) / int(cora_graph.train_mask.sum())\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(cora_graph)\n",
    "        pred_val = out.argmax(dim=1)\n",
    "\n",
    "        correct_val = (\n",
    "            pred_val[cora_graph.val_mask]\n",
    "            == cora_graph.y[cora_graph.val_mask]\n",
    "        ).sum()\n",
    "        acc_val = int(correct_val) / int(cora_graph.val_mask.sum())\n",
    "\n",
    "    # Save best model\n",
    "    if acc_val > best_acc_val:\n",
    "        best_acc_val = acc_val\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # Print progress\n",
    "    if epoch % 10 == 0:\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d} | \"\n",
    "            f\"Loss: {loss.item():.4f} | \"\n",
    "            f\"Train Acc: {acc_train:.4f} | \"\n",
    "            f\"Val Acc: {acc_val:.4f}\"\n",
    "        )\n",
    "\n",
    "# Load best model after training\n",
    "model.load_state_dict(best_model_state)\n",
    "print(f\"Best Validation Accuracy: {best_acc_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "\n",
    "def visualize_tsne(emb, labels, seed=42):\n",
    "    X = emb.detach().cpu().numpy().astype(np.float32)\n",
    "    y = labels.detach().cpu().numpy()\n",
    "\n",
    "    n = X.shape[0]\n",
    "    perplexity = min(30, max(5, (n - 1) // 3))\n",
    "\n",
    "    tsne = TSNE(\n",
    "        n_components=2,\n",
    "        init=\"pca\",\n",
    "        learning_rate=\"auto\",\n",
    "        perplexity=perplexity,\n",
    "        random_state=seed,\n",
    "        method=\"barnes_hut\",\n",
    "        angle=0.5\n",
    "    )\n",
    "\n",
    "    Z = tsne.fit_transform(X)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.scatter(Z[:, 0], Z[:, 1], s=15, c=y, cmap=\"Set2\")\n",
    "    plt.title(\"t-SNE of node embeddings\")\n",
    "    plt.show()\n",
    "\n",
    "# Run safely\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data)\n",
    "\n",
    "visualize_tsne(out, data.y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model obtained from validation\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# ---- Test Accuracy ----\n",
    "with torch.no_grad():\n",
    "    pred = model(data).argmax(dim=1)\n",
    "\n",
    "    correct = (\n",
    "        pred[data.test_mask]\n",
    "        == data.y[data.test_mask]\n",
    "    ).sum().item()\n",
    "\n",
    "    test_acc = correct / data.test_mask.sum().item()\n",
    "\n",
    "print(f\"Best Test Accuracy: {test_acc:.3f}\")\n",
    "\n",
    "# ---- Embedding Visualization ----\n",
    "with torch.no_grad():\n",
    "    out = model(data)\n",
    "\n",
    "print(\"Node embedding shape:\", out.shape)\n",
    "\n",
    "# Visualize embeddings using true labels\n",
    "visualize(out, color=data.y.cpu())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Node classification using Graph Attention network\n",
    "\n",
    "This class defines a Graph Attention Network (GAT), which is a type of graph neural network that uses attention mechanisms to decide how important each neighboring node is during message passing.\n",
    "The `__init__` method sets up the model architecture. The first `GATConv` layer applies multi-head attention, meaning that multiple attention mechanisms operate in parallel to capture different aspects of neighborhood information. Each head learns how strongly a node should attend to its neighbors. The outputs of these heads are concatenated, which is why the input size of the second layer is `hidden_channels × num_heads`.\n",
    "The second GATConv layer uses a single attention head and produces the final output, where each node receives a vector of length equal to the number of classes. This layer is typically used for node classification.\n",
    "In the forward method, node features (`x`) and graph connectivity (edge_index) are extracted from the input graph. Dropout is applied to reduce overfitting. The first attention layer aggregates neighbor information using attention scores, followed by an ELU activation function to introduce nonlinearity. Another dropout layer is applied before the final attention layer, which outputs class scores for each node.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels,\n",
    "        output_channels,\n",
    "        hidden_channels=8,\n",
    "        num_heads=8\n",
    "    ):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(123456)\n",
    "\n",
    "        # First GAT layer with multi-head attention\n",
    "        self.gatconv1 = GATConv(\n",
    "            in_channels=input_channels,\n",
    "            out_channels=hidden_channels,\n",
    "            heads=num_heads\n",
    "        )\n",
    "\n",
    "        # Second GAT layer with a single attention head\n",
    "        self.gatconv2 = GATConv(\n",
    "            in_channels=hidden_channels * num_heads,\n",
    "            out_channels=output_channels,\n",
    "            heads=1\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # Dropout on input features\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "\n",
    "        # First GAT layer + nonlinearity\n",
    "        x = self.gatconv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "\n",
    "        # Dropout before final layer\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "\n",
    "        # Second GAT layer (output layer)\n",
    "        x = self.gatconv2(x, edge_index)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Select device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the Cora graph and move it to the selected device\n",
    "cora_graph = cora_dataset[0].to(device)\n",
    "\n",
    "# Define model input and output dimensions\n",
    "input_channels = cora_dataset.num_features\n",
    "output_channels = cora_dataset.num_classes\n",
    "\n",
    "# Initialize the GCN model\n",
    "model = GCN(\n",
    "    input_channels=input_channels,\n",
    "    output_channels=output_channels\n",
    ").to(device)\n",
    "\n",
    "# Print model architecture\n",
    "print(model)\n",
    "\n",
    "# Print total number of trainable parameters\n",
    "print(\n",
    "    \"Number of parameters:\",\n",
    "    sum(p.numel() for p in model.parameters())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize(h, color):\n",
    "    # Reduce high-dimensional embeddings to 2D using t-SNE\n",
    "    z = TSNE(n_components=2).fit_transform(\n",
    "        h.detach().cpu().numpy()\n",
    "    )\n",
    "\n",
    "    # Plot settings\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    # Scatter plot of nodes\n",
    "    plt.scatter(\n",
    "        z[:, 0],\n",
    "        z[:, 1],\n",
    "        s=70,\n",
    "        c=color,\n",
    "        cmap=\"Set2\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Use the trained model to get node embeddings\n",
    "model.eval()\n",
    "out = model(cora_graph)\n",
    "\n",
    "print(\"Node embedding shape:\", out.shape)\n",
    "\n",
    "# Visualize embeddings using true labels as colors\n",
    "visualize(out, color=cora_graph.y.cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Initialize the GAT model\n",
    "model = GAT(\n",
    "    input_channels=input_channels,\n",
    "    output_channels=output_channels\n",
    ").to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.005,\n",
    "    weight_decay=5e-4\n",
    ")\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# ---------------- Training Loop ----------------\n",
    "num_epochs = 200\n",
    "cora_graph = cora_dataset[0].to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    out = model(cora_graph)\n",
    "\n",
    "    # Compute loss on training nodes only\n",
    "    loss = criterion(\n",
    "        out[cora_graph.train_mask],\n",
    "        cora_graph.y[cora_graph.train_mask]\n",
    "    )\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Accuracy + Progress Printing (Every 10 Epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on the training data\n",
    "pred_train = out.argmax(dim=1)\n",
    "\n",
    "# Count correct predictions only on training nodes\n",
    "correct_train = (\n",
    "    pred_train[cora_graph.train_mask]\n",
    "    == cora_graph.y[cora_graph.train_mask]\n",
    ").sum()\n",
    "\n",
    "# Compute training accuracy\n",
    "acc_train = int(correct_train) / int(cora_graph.train_mask.sum())\n",
    "\n",
    "# Print training progress every 10 epochs\n",
    "if (epoch + 1) % 10 == 0:\n",
    "    print(\n",
    "        f\"Epoch: {epoch + 1:03d}, \"\n",
    "        f\"Train Loss: {loss:.3f}, \"\n",
    "        f\"Train Acc: {acc_train:.3f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient computation for evaluation\n",
    "with torch.no_grad():\n",
    "    # Get predicted class labels for all nodes\n",
    "    pred = model(cora_graph).argmax(dim=1)\n",
    "\n",
    "    # Count correct predictions on test nodes only\n",
    "    correct = (\n",
    "        pred[cora_graph.test_mask]\n",
    "        == cora_graph.y[cora_graph.test_mask]\n",
    "    ).sum().item()\n",
    "\n",
    "    # Compute test accuracy\n",
    "    test_acc = correct / cora_graph.test_mask.sum().item()\n",
    "\n",
    "# Print test accuracy\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## choose the best model based on validation data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import copy\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "# Keep track of best validation accuracy and best model state\n",
    "best_acc_val = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # -------- TRAINING PHASE --------\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = model(cora_graph)\n",
    "\n",
    "    # Training loss (only on training nodes)\n",
    "    loss = criterion(\n",
    "        out[cora_graph.train_mask],\n",
    "        cora_graph.y[cora_graph.train_mask]\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Training accuracy\n",
    "    pred_train = out.argmax(dim=1)\n",
    "    correct_train = (\n",
    "        pred_train[cora_graph.train_mask]\n",
    "        == cora_graph.y[cora_graph.train_mask]\n",
    "    ).sum()\n",
    "    acc_train = int(correct_train) / int(cora_graph.train_mask.sum())\n",
    "\n",
    "    # -------- VALIDATION PHASE --------\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_val = model(cora_graph).argmax(dim=1)\n",
    "        correct_val = (\n",
    "            pred_val[cora_graph.val_mask]\n",
    "            == cora_graph.y[cora_graph.val_mask]\n",
    "        ).sum()\n",
    "        acc_val = int(correct_val) / int(cora_graph.val_mask.sum())\n",
    "\n",
    "    # -------- SAVE BEST MODEL --------\n",
    "    if acc_val > best_acc_val:\n",
    "        best_acc_val = acc_val\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # -------- LOGGING --------\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1:03d}, \"\n",
    "            f\"Train Loss: {loss:.3f}, \"\n",
    "            f\"Train Acc: {acc_train:.3f}, \"\n",
    "            f\"Val Acc: {acc_val:.3f}\"\n",
    "        )\n",
    "\n",
    "# Load the best model after training\n",
    "model.load_state_dict(best_model_state)\n",
    "print(f\"Best Validation Accuracy: {best_acc_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Best Model, Test Accuracy, and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best validation accuracy and load best model\n",
    "print(\"Best validation accuracy:\", best_acc_val)\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "# -------- TEST EVALUATION --------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(cora_graph).argmax(dim=1)\n",
    "\n",
    "    correct = (\n",
    "        pred[cora_graph.test_mask]\n",
    "        == cora_graph.y[cora_graph.test_mask]\n",
    "    ).sum().item()\n",
    "\n",
    "    test_acc = correct / cora_graph.test_mask.sum().item()\n",
    "\n",
    "print(f\"Best Test Accuracy: {test_acc:.3f}\")\n",
    "\n",
    "# -------- NODE EMBEDDING VISUALIZATION --------\n",
    "model.eval()\n",
    "out = model(cora_graph)\n",
    "\n",
    "print(\"Node embedding shape:\", out.shape)\n",
    "\n",
    "visualize(out, color=cora_graph.cpu().y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Classification using GNN\n",
    "using the node and edge features to make predictions about graph itself.\n",
    "In our example we are going to predict wheter a protein is enzyme or not.\n",
    "\n",
    "The node features and edge features are aggregated to gether in a meaningful way to generate an embedding representation for graph as whole and then you can use that embedding for protein classification. This embedding have local neighborhood information about each target node or each edge. This can be done by gathering embedding across all the nodes and edges of the graph and then using pooling layers to aggregate this information for the graph.\n",
    "\n",
    "You can take that representaion and passed it through linear layer and categorize the entire graph.\n",
    "![GraphClassification](images/GNN_Gclass.png)\n",
    "\n",
    "\n",
    "The **PROTEINS** dataset is a graph classification dataset commonly used to evaluate Graph Neural Networks (GNNs).\n",
    "\n",
    "- Each graph represents a protein\n",
    "\n",
    "- Nodes correspond to amino acids\n",
    "\n",
    "- Edges represent spatial or chemical interactions between amino acids\n",
    "\n",
    "- Node features describe properties of each amino acid\n",
    "\n",
    "- Labels indicate the functional class of the protein (binary classification)\n",
    "\n",
    "This dataset is part of the **TU Dataset collection** and is widely used for benchmarking GNN models on biological graph data.\n",
    "\n",
    "Task: Given a protein graph, predict wheter it is an enzyme or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROTEINS(1113)\n",
      "Number of graphs: 1113\n",
      "Number of node features: 3\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import TUDataset\n",
    "\n",
    "# Load the PROTEINS dataset\n",
    "proteins_dataset = TUDataset(\n",
    "    root=\"data/TUDataset\",\n",
    "    name=\"PROTEINS\"\n",
    ")\n",
    "\n",
    "# Print basic dataset information\n",
    "print(proteins_dataset)\n",
    "print(\"Number of graphs:\", len(proteins_dataset))\n",
    "print(\"Number of node features:\", proteins_dataset.num_features)\n",
    "print(\"Number of classes:\", proteins_dataset.num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node features are node degee, clustering coefficient, and node label that is a categorical feature represent the type of amino acid.\n",
    "\n",
    "The class are Enzyme (1) or non-enzyme(0).\n",
    "\n",
    "Now lets look at the structure of graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 162], x=[42, 3], y=[1])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proteins_graph1 = proteins_dataset[0]\n",
    "proteins_graph1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This protein has 42 aminoa cids and 162 edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 162)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proteins_graph1.num_nodes, proteins_graph1.num_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proteins_graph1.has_isolated_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proteins_graph1.has_self_loops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proteins_graph1.is_undirected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 700\n",
      "Number of validation graphs: 200\n",
      "Number of test graphs: 213\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(12345)\n",
    "\n",
    "protein_dataset = proteins_dataset.shuffle()\n",
    "\n",
    "# Now the data is shuffled and we can slice the data to split in to train and test and validation\n",
    "train_dataset = protein_dataset[:700]\n",
    "val_dataset = protein_dataset[700:900]\n",
    "test_dataset = protein_dataset[900:]\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f\"Number of validation graphs: {len(val_dataset)}\")\n",
    "print(f\"Number of test graphs: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batching Graphs with DataLoader (PyTorch Geometric)\n",
    "\n",
    "In graph learning, **each data sample** is a graph, not a single vector.\n",
    "The DataLoader in `PyTorch Geometric` allows us to combine **multiple graphs into one mini-batch** so they can be processed efficiently by a GNN.\n",
    "\n",
    "**What happens in mini-batching?**\n",
    "\n",
    "- Multiple graphs are **merged into one large disconnected graph**\n",
    "\n",
    "- Node features (`x`) are concatenated\n",
    "\n",
    "- Edge indices (`edge_index`) are shifted automatically\n",
    "\n",
    "- A batch vector is added to indicate **which node belongs to which graph**\n",
    "\n",
    "\n",
    "**Key points in the code**\n",
    "\n",
    "\n",
    "- `batch_size=64` → 64 graphs per mini-batch\n",
    "\n",
    "- `shuffle=True` (training only) → improves generalization\n",
    "\n",
    "- `data.num_graphs` → number of graphs in the batch\n",
    "\n",
    "- `data` → a single `Batch` object containing all graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(edge_index=[2, 9978], x=[2714, 3], y=[64], batch=[2714], ptr=[65])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Create DataLoaders for mini-batching graphs\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Inspect one mini-batch from the training loader\n",
    "for step, data in enumerate(train_loader):\n",
    "    print(f\"Step {step + 1}\")\n",
    "    print(\"=======\")\n",
    "    print(f\"Number of graphs in the current batch: {data.num_graphs}\")\n",
    "    print(data)\n",
    "    print()\n",
    "    break  # show only the first batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across 64 graph bathes in train we have 2714 nodes and 9978 edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Graph classification as opposed to node classification many of the steps remain the same but there are a few differences the first thing is you generate embeddings for each node using the usual message passing techniques so you perform multiple rounds of message passing that is you aggregate information from the neighboring notes for every note in the graph once you have the updated note embeddings for every note in the graph you aggregate node embeddings across the entire graph structure into a unified if you haven't graph embedding representing the entire graph you pass that through a linear classifier to categorize or classify that particular graph let's set up a graph neural network to perform exactly these three steps this neural network class is called gcn so it's a convolutional network because we still use message passing in aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN model for graph classification (PROTEINS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = GCNConv(\n",
    "            proteins_dataset.num_node_features,\n",
    "            hidden_channels\n",
    "        )\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.lin = Linear(\n",
    "            hidden_channels,\n",
    "            proteins_dataset.num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Node-level embeddings\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Graph-level readout\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # 3. Classification\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model initialization and parameter count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(3, 64)\n",
      "  (conv2): GCNConv(64, 64)\n",
      "  (conv3): GCNConv(64, 64)\n",
      "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "Number of parameters: 8706\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GCN(hidden_channels=64).to(device)\n",
    "print(model)\n",
    "\n",
    "print(\n",
    "    \"Number of parameters:\",\n",
    "    sum(p.numel() for p in model.parameters())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "criterion =  torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x,data.edge_index, data.batch)\n",
    "        \n",
    "        loss= criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        \n",
    "        out = model(data.x,data.edge_index, data.batch)\n",
    "        pred= out.argmax(dim = 1)\n",
    "        correct = int((pred == data.y).sum())\n",
    "        \n",
    "    return correct/len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Train Acc: 0.0614, Val Acc: 0.0200\n",
      "Epoch: 020, Train Acc: 0.0600, Val Acc: 0.0250\n",
      "Epoch: 030, Train Acc: 0.0657, Val Acc: 0.0200\n",
      "Epoch: 040, Train Acc: 0.0629, Val Acc: 0.0200\n",
      "Epoch: 050, Train Acc: 0.0600, Val Acc: 0.0200\n",
      "Epoch: 060, Train Acc: 0.0543, Val Acc: 0.0150\n",
      "Epoch: 070, Train Acc: 0.0671, Val Acc: 0.0250\n",
      "Epoch: 080, Train Acc: 0.0543, Val Acc: 0.0150\n",
      "Epoch: 090, Train Acc: 0.0614, Val Acc: 0.0150\n",
      "Epoch: 100, Train Acc: 0.0671, Val Acc: 0.0200\n",
      "Epoch: 110, Train Acc: 0.0686, Val Acc: 0.0200\n",
      "Epoch: 120, Train Acc: 0.0657, Val Acc: 0.0200\n",
      "Epoch: 130, Train Acc: 0.0700, Val Acc: 0.0200\n",
      "Epoch: 140, Train Acc: 0.0614, Val Acc: 0.0150\n",
      "Epoch: 150, Train Acc: 0.0643, Val Acc: 0.0200\n",
      "Epoch: 160, Train Acc: 0.0586, Val Acc: 0.0200\n",
      "Epoch: 170, Train Acc: 0.0671, Val Acc: 0.0150\n",
      "Epoch: 180, Train Acc: 0.0600, Val Acc: 0.0200\n",
      "Epoch: 190, Train Acc: 0.0571, Val Acc: 0.0150\n",
      "Epoch: 200, Train Acc: 0.0614, Val Acc: 0.0150\n",
      "Best validation accuracy:  0.025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "import torch\n",
    "\n",
    "# Model, optimizer, and loss function\n",
    "model = GCN(hidden_channels=64).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train for one epoch\n",
    "    train()\n",
    "\n",
    "    # Evaluate on training and validation sets\n",
    "    train_acc = test(train_loader)\n",
    "    val_acc = test(val_loader)\n",
    "\n",
    "    # Save the best model based on validation accuracy\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # Print progress every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(\n",
    "            f\"Epoch: {epoch + 1:03d}, \"\n",
    "            f\"Train Acc: {train_acc:.4f}, \"\n",
    "            f\"Val Acc: {val_acc:.4f}\"\n",
    "        )\n",
    "print(\"Best validation accuracy: \", best_val_acc)\n",
    "model.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Autoencoder\n",
    "\n",
    "specialized type of auto encoder designed to work with graph structured data. useful for tasks such as node clustering, link prediction, and graph generation.\n",
    "\n",
    "![GraphAutoencoder](images/GNN_autoEncoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citeseer()\n",
      "Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327])\n",
      "x (node features): torch.Size([3327, 3703])\n",
      "edge_index (edges): torch.Size([2, 9104])\n",
      "y (labels): torch.Size([3327])\n",
      "train/val/test masks: torch.Size([3327]) torch.Size([3327]) torch.Size([3327])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "citeseer_dataset = Planetoid(root = \"Citeser_dataset\", name = \"Citeseer\")\n",
    "\n",
    "citeseer_dataset\n",
    "# The Planetoid datasets (Cora/CiteSeer/PubMed) each contain a single graph\n",
    "data = citeseer_dataset[0]\n",
    "\n",
    "print(citeseer_dataset)\n",
    "print(data)\n",
    "\n",
    "# Optional: print key shapes\n",
    "print(\"x (node features):\", data.x.shape)          # [num_nodes, num_node_features]\n",
    "print(\"edge_index (edges):\", data.edge_index.shape) # [2, num_edges]\n",
    "print(\"y (labels):\", data.y.shape)                  # [num_nodes]\n",
    "print(\"train/val/test masks:\", data.train_mask.shape, data.val_mask.shape, data.test_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CiteSeer is a classic citation network dataset used for node classification with GNNs.\n",
    "\n",
    "Nodes = research papers\n",
    "\n",
    "Edges = citation links between papers (paper A cites paper B)\n",
    "\n",
    "Node features (data.x) = a bag-of-words style vector (each row represents one paper, each column represents a word/term feature).\n",
    "\n",
    "Node labels (data.y) = the research topic/category of each paper (one label per node).\n",
    "\n",
    "Masks (train_mask, val_mask, test_mask) = predefined splits that tell you which nodes to use for training, validation, and testing.\n",
    "\n",
    "In PyTorch Geometric, Planetoid(..., name=\"CiteSeer\") downloads and prepares the dataset, and citeseer_dataset[0] returns the single graph stored in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Validation Data\n",
      "Data(x=[3327, 3703], edge_index=[2, 6374], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327], edge_label=[910], edge_label_index=[2, 910])\n",
      "--------------------------------------------------\n",
      "Test Data\n",
      "Data(x=[3327, 3703], edge_index=[2, 7284], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327], edge_label=[1820], edge_label_index=[2, 1820])\n",
      "--------------------------------------------------\n",
      "Train Data\n",
      "Data(x=[3327, 3703], edge_index=[2, 6374], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327], edge_label=[3187], edge_label_index=[2, 3187])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "# Device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define transformations\n",
    "transform = T.Compose([\n",
    "    T.NormalizeFeatures(),          # Normalize node feature vectors\n",
    "    T.ToDevice(device),              # Move data to CPU/GPU\n",
    "    T.RandomLinkSplit(\n",
    "        num_val=0.10,                # 10% edges for validation\n",
    "        num_test=0.20,               # 20% edges for testing\n",
    "        neg_sampling_ratio=1.0,      # Equal number of negative edges\n",
    "        is_undirected=True,          # Treat graph as undirected\n",
    "        add_negative_train_samples=False\n",
    "    )\n",
    "])\n",
    "\n",
    "# Load CiteSeer dataset with transform\n",
    "citeseer_dataset = Planetoid(\n",
    "    root=\"Citeseer_dataset\",\n",
    "    name=\"Citeseer\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# After RandomLinkSplit, indexing returns (train, val, test)\n",
    "train_data, val_data, test_data = citeseer_dataset[0]\n",
    "\n",
    "# Print dataset splits\n",
    "print(\"-\" * 50)\n",
    "print(\"Validation Data\")\n",
    "print(val_data)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Test Data\")\n",
    "print(test_data)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Train Data\")\n",
    "print(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3327, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Module\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# -------------------------------\n",
    "# Graph Autoencoder Model\n",
    "# -------------------------------\n",
    "class AutoEncoder(Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder (GCN layers)\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x  # latent node embeddings Z\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        # Inner product decoder\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        # Reconstruct full adjacency matrix\n",
    "        prob_adj = z @ z.t()\n",
    "        prob_adj = prob_adj.sigmoid()\n",
    "\n",
    "        # Return only very confident edges\n",
    "        return (prob_adj > 0.99).nonzero(as_tuple=False).t()\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Model setup\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoEncoder(\n",
    "    in_channels=citeseer_dataset.num_features,\n",
    "    hidden_channels=128,\n",
    "    out_channels=64\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# -------------------------------\n",
    "# Encode node embeddings\n",
    "# -------------------------------\n",
    "z = model.encode(train_data.x, train_data.edge_index)\n",
    "print(z.shape)  # [num_nodes, latent_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3327, 64])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = model.encode(train_data.x , train_data.edge_index)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss: 0.5807, Val AUC: 0.7597, Test AUC: 0.7830\n",
      "Epoch: 020, Loss: 0.5213, Val AUC: 0.8127, Test AUC: 0.8321\n",
      "Epoch: 030, Loss: 0.4988, Val AUC: 0.8564, Test AUC: 0.8685\n",
      "Epoch: 040, Loss: 0.4753, Val AUC: 0.8449, Test AUC: 0.8626\n",
      "Epoch: 050, Loss: 0.4546, Val AUC: 0.8560, Test AUC: 0.8842\n",
      "Epoch: 060, Loss: 0.4503, Val AUC: 0.8544, Test AUC: 0.8868\n",
      "Epoch: 070, Loss: 0.4416, Val AUC: 0.8642, Test AUC: 0.8968\n",
      "Epoch: 080, Loss: 0.4393, Val AUC: 0.8665, Test AUC: 0.9009\n",
      "Epoch: 090, Loss: 0.4368, Val AUC: 0.8595, Test AUC: 0.8947\n",
      "Epoch: 100, Loss: 0.4365, Val AUC: 0.8604, Test AUC: 0.8935\n",
      "Final Test AUC: 0.9008\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------------------\n",
    "# 2) Instantiate model/optimizer/criterion\n",
    "# ----------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoEncoder(\n",
    "    citeseer_dataset.num_features,\n",
    "    hidden_channels=128,\n",
    "    out_channels=64\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Train step\n",
    "# -----------------------------\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "    # train_data.edge_label_index shape: (2, num_train_edges)\n",
    "    # neg_edge_index shape: (2, num_train_edges)\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index,\n",
    "        num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1)\n",
    "    )\n",
    "\n",
    "    # edge_label_index shape: (2, num_train_edges * 2)\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1\n",
    "    )\n",
    "\n",
    "    # edge_label shape: (num_train_edges * 2)\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Test step (ROC-AUC)\n",
    "# -----------------------------\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "\n",
    "    return roc_auc_score(\n",
    "        data.edge_label.cpu().numpy(),\n",
    "        out.cpu().numpy()\n",
    "    )\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Training loop\n",
    "# -----------------------------\n",
    "best_val_auc = final_test_auc = 0\n",
    "\n",
    "for epoch in range(100):\n",
    "    loss = train()\n",
    "\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1:03d}, Loss: {loss:.4f}, \"\n",
    "            f\"Val AUC: {val_auc:.4f}, Test AUC: {test_auc:.4f}\"\n",
    "        )\n",
    "\n",
    "print(f\"Final Test AUC: {final_test_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3211, -0.0390, -0.3111,  ...,  0.0044,  0.1391,  0.0499],\n",
       "        [ 0.0751,  0.0498,  0.2231,  ..., -0.2271, -0.1097,  0.0576],\n",
       "        [-0.2011, -0.1242, -0.1278,  ...,  0.1155,  0.0714, -0.0581],\n",
       "        ...,\n",
       "        [-0.4045, -0.0919, -0.0364,  ..., -0.1427,  0.0813,  0.1555],\n",
       "        [-0.1620,  0.1902,  0.0524,  ..., -0.0284, -0.1536,  0.0007],\n",
       "        [-0.1588, -0.0428,  0.1831,  ...,  0.0700, -0.0832, -0.0629]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = model.encode(test_data.x, test_data.edge_index)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3327, 64])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4542])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 80])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_loops = final_edge_index[0] == final_edge_index[1]\n",
    "\n",
    "filterd_edges = final_edge_index[:,self_loops]\n",
    "filterd_edges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7284])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

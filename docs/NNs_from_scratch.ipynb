{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ecaqr9YkWpbL"
   },
   "source": [
    "# Multilayer Perceptron (MLP) From Scratch (NumPy)\n",
    "\n",
    "\n",
    "\n",
    "In this section, we develop the fundamental components of a feedforward neural network from first principles. Each component is derived mathematically and implemented explicitly using NumPy. No automatic differentiation or high-level abstractions are used; all gradients are computed manually to make the learning dynamics fully transparent.\n",
    "\n",
    "We begin by defining a common interface for network layers, then implement linear transformations and nonlinear activation functions. Together, these components form the basis of forward propagation and backpropagation.\n",
    "## Mathematical Equations, Computational Graphs, and Backpropagation\n",
    "\n",
    "This notebook builds an MLP by defining **Layer** objects (Dense, ReLU, etc.) and composing them.\n",
    "\n",
    "We cover:\n",
    "- Forward equations\n",
    "- Backward gradient equations\n",
    "- Computational graph intuition\n",
    "- Softmax + cross-entropy loss\n",
    "- A minimal training loop\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Layer Abstraction\n",
    "\n",
    "All neural network layers share a common structure:\n",
    "\n",
    "- a **forward pass**, which computes the output of the layer,\n",
    "\n",
    "- a **backward pass**, which computes gradients using the chain rule,\n",
    "\n",
    "- and, for parameterized layers, a **parameter update step**.\n",
    "\n",
    "The `Layer` class serves as a conceptual base class that defines this interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.inp = None\n",
    "        self.out = None\n",
    "\n",
    "    def __call__(self, inp: np.ndarray) -> np.ndarray:\n",
    "        return self.forward(inp)\n",
    "\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def step(self, lr: float) -> None:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each concrete layer implements its own forward and backward computations. During training, layers are executed sequentially in the forward direction and in reverse order during backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear (Fully Connected) Layers\n",
    "\n",
    "The `Linear` class implements the fully connected (or dense) layer of a neural network, which performs a linear transformation on the input:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{x} \\cdot \\mathbf{W} + \\mathbf{b}$$\n",
    "\n",
    "where:\n",
    "- $x \\in \\mathbb{R}^{N \\times d_{\\text{in}}}$ is the input batch,\n",
    "\n",
    "- $W \\in \\mathbb{R}^{d_{\\text{in}} \\times d_{\\text{out}}}$ is the weight matrix,\n",
    "\n",
    "- $b \\in \\mathbb{R}^{1 \\times d_{\\text{out}}}$ is the bias vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phTfqRsVIZKA"
   },
   "source": [
    "**Initialization**\n",
    "\n",
    "Weights are initialized using **He initialization**, which scales the variance to stabilize gradients in deep networks:\n",
    "\n",
    "$$W_{ij} \\sim \\mathcal{N}\\!\\left(0, \\frac{2}{d_{\\text{in}}}\\right)$$\n",
    "\n",
    "\n",
    "Biases are initialized to zero.\n",
    "- `self.w`: Represents the weight matrix of shape `(in_dim, out_dim)`, initialized using small random values.\n",
    "- `self.b`: Bias vector of shape `(1, out_dim)`, initialized to zeros.\n",
    "- `self.dw` and `self.db`: These store the computed gradients of weights and biases during backpropagation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Forward Pass**\n",
    "- Given an input batch `inp`, the forward pass computes:\n",
    "  \n",
    "$$\\mathbf{out} = \\mathbf{inp} \\cdot \\mathbf{W} + \\mathbf{b}$$\n",
    "where:\n",
    "  - `inp`: Input matrix of shape `(batch_size, in_dim)`\n",
    "  - `self.w`: Weight matrix of shape `(in_dim, out_dim)`\n",
    "  -\t`self.b`: Bias matrix of shape `(1, out_dim)`\n",
    "-\tThe result is a matrix out of shape `(batch_size, out_dim)`.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Backward Pass**\n",
    "- The backward pass computes gradients needed for updating the weights and biases. Given the upstream gradient `up_grad` (from the loss with respect to the output of this layer), we calculate:\n",
    "\n",
    "Given the upstream gradient $\\frac{\\partial L}{\\partial \\text{out}}$,\n",
    "the gradients are:\n",
    "\n",
    "- **Weights** $\\frac{\\partial \\mathbf{L}}{\\partial W}= x^{\\top} \\frac{\\partial L}{\\partial \\text{out}}$.\n",
    "\n",
    "- **Biases** $\\frac{\\partial L}{\\partial b}\n",
    "= \\sum_{i=1}^{N} \\frac{\\partial L}{\\partial \\text{out}_i}.$\n",
    "\n",
    "- **Input (for propagation to the previous layer)** $\\frac{\\partial L}{\\partial x}\n",
    "= \\frac{\\partial L}{\\partial \\text{out}} W^{\\top}.$\n",
    "\n",
    "  - Gradient w.r.t. weights (`self.dw`):\n",
    "    $$ \\frac{\\partial L}{\\partial W} = \\mathbf{inp}^T \\cdot \\text{up_grad} $$\n",
    "  - Gradient w.r.t. biases (`self.db`):\n",
    "    $$\\frac{\\partial L}{\\partial b} = \\sum \\text{up_grad} \\text{ (summed across batch)}$$\n",
    "  - Gradient to propagate to the previous layer (`down_grad`):\n",
    "    $$\\text{down_grad} = \\text{up_grad} \\cdot W^T$$\n",
    "- This allows the gradient to flow backward to earlier layers.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Step Method**\n",
    "- Updates the weights and biases using the computed gradients and learning rate (`lr`):\n",
    "    $$W = W - lr \\cdot \\frac{\\partial L}{\\partial W}$$\n",
    "    $$b = b - lr \\cdot \\frac{\\partial L}{\\partial b}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "PqFOwWLZVPI0"
   },
   "outputs": [],
   "source": [
    "# imports required packages\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import trange\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "The following helper functions provide supporting utilities for visualization, data preparation, and evaluation. They are intentionally separated from the neural network implementation to keep the core learning algorithms clear and focused.\n",
    "\n",
    "These functions rely on standard scientific Python libraries—primarily NumPy, Matplotlib, and selected utilities from scikit-learn—for tasks such as dataset loading, plotting training curves, and computing confusion matrices. While these tools simplify experimentation and analysis, they are not used to implement the neural network itself, which is built entirely from first principles.\n",
    "\n",
    "Readers may treat these helpers as interchangeable infrastructure: they can be modified, replaced, or omitted without affecting the underlying learning algorithm. The emphasis of this chapter remains on understanding neural network mechanics rather than visualization or data-handling boilerpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "form",
    "id": "9AKM0RgiVSOQ"
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def plot_training(losses):\n",
    "    \"\"\"Plot training loss over epochs.\"\"\"\n",
    "    plt.plot(losses)\n",
    "    plt.title(\"Training loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names, kept_classes, normalize=True):\n",
    "    \"\"\"\n",
    "    Plot a (optionally normalized) confusion matrix for selected classes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true, y_pred : array-like\n",
    "        Ground-truth and predicted labels (integer-encoded).\n",
    "    class_names : list[str]\n",
    "        Names for the original dataset classes (e.g., Fashion-MNIST label names).\n",
    "    kept_classes : list[int]\n",
    "        Which original classes are present in this experiment.\n",
    "    normalize : bool\n",
    "        If True, normalize each row to sum to 1.\n",
    "    \"\"\"\n",
    "    labels = [class_names[i] for i in kept_classes]\n",
    "    conf_mat = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    if normalize:\n",
    "        row_sums = conf_mat.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1  # avoid division by zero\n",
    "        disp_mat = conf_mat / row_sums\n",
    "    else:\n",
    "        disp_mat = conf_mat\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(disp_mat)\n",
    "    ax.set_title(\"Confusion Matrix\" + (\" (normalized)\" if normalize else \"\"))\n",
    "    ax.set_xlabel(\"Predictions\")\n",
    "    ax.set_ylabel(\"Labels\")\n",
    "    ax.set_xticks(range(len(labels)))\n",
    "    ax.set_yticks(range(len(labels)))\n",
    "    ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "    ax.set_yticklabels(labels)\n",
    "    fig.colorbar(im, ax=ax)\n",
    "\n",
    "    # Annotate with raw counts (more interpretable than normalized values)\n",
    "    for i in range(conf_mat.shape[0]):\n",
    "        for j in range(conf_mat.shape[1]):\n",
    "            ax.text(j, i, int(conf_mat[i, j]), va=\"center\", ha=\"center\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_data(kept_classes, test_size=10_000, random_state=42):\n",
    "    \"\"\"\n",
    "    Load Fashion-MNIST from OpenML, filter to a subset of classes,\n",
    "    normalize pixels to [-1, 1], and relabel classes to 0..(C-1).\n",
    "    \"\"\"\n",
    "    fashion_mnist = fetch_openml(\"Fashion-MNIST\", parser=\"auto\")\n",
    "    x = fashion_mnist[\"data\"].to_numpy()\n",
    "    y = fashion_mnist[\"target\"].astype(int).to_numpy()\n",
    "\n",
    "    # Filter classes\n",
    "    mask = np.isin(y, kept_classes)\n",
    "    x, y = x[mask], y[mask]\n",
    "\n",
    "    # Normalize pixels to [-1, 1]\n",
    "    x = ((x / 255.0) - 0.5) * 2.0\n",
    "\n",
    "    # Relabel to 0..C-1 (stable mapping based on kept_classes order)\n",
    "    mapping = {cls: idx for idx, cls in enumerate(kept_classes)}\n",
    "    y = np.array([mapping[label] for label in y], dtype=int)\n",
    "\n",
    "    return train_test_split(x, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "\n",
    "\n",
    "def onehot_encoder(y, num_labels):\n",
    "    \"\"\"Convert integer labels to one-hot encoding.\"\"\"\n",
    "    one_hot = np.zeros((y.size, num_labels), dtype=int)\n",
    "    one_hot[np.arange(y.size), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def plot_batch_size(vanilla, stochastic, mini_batch):\n",
    "    \"\"\"\n",
    "    Compare training loss and test accuracy for different batch-size regimes.\n",
    "\n",
    "    Expected inputs:\n",
    "      vanilla   = (epoch_losses, batch_losses, test_accuracies)\n",
    "      stochastic= (epoch_losses, batch_losses, test_accuracies)\n",
    "      mini_batch= (epoch_losses, batch_losses, test_accuracies)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "    # Training loss (per epoch)\n",
    "    axes[0, 0].plot(vanilla[0], label=\"Gradient Descent\")\n",
    "    axes[0, 0].plot(stochastic[0], label=\"Stochastic Gradient Descent\")\n",
    "    axes[0, 0].plot(mini_batch[0], label=\"Mini-Batch Gradient Descent\")\n",
    "    axes[0, 0].set_xlabel(\"Epoch\")\n",
    "    axes[0, 0].set_ylabel(\"Loss\")\n",
    "    axes[0, 0].set_title(\"Training Loss\")\n",
    "    axes[0, 0].legend()\n",
    "\n",
    "    # Test accuracy (per epoch)\n",
    "    axes[0, 1].plot(vanilla[2], label=\"Gradient Descent\")\n",
    "    axes[0, 1].plot(stochastic[2], label=\"Stochastic Gradient Descent\")\n",
    "    axes[0, 1].plot(mini_batch[2], label=\"Mini-Batch Gradient Descent\")\n",
    "    axes[0, 1].set_xlabel(\"Epoch\")\n",
    "    axes[0, 1].set_ylabel(\"Accuracy\")\n",
    "    axes[0, 1].set_title(\"Test Accuracy\")\n",
    "    axes[0, 1].legend()\n",
    "\n",
    "    # Batch loss traces\n",
    "    axes[1, 0].plot(stochastic[1])\n",
    "    axes[1, 0].set_xlabel(\"Batch\")\n",
    "    axes[1, 0].set_ylabel(\"Loss\")\n",
    "    axes[1, 0].set_title(\"SGD Batch Loss\")\n",
    "\n",
    "    axes[1, 1].plot(mini_batch[1])\n",
    "    axes[1, 1].set_xlabel(\"Batch\")\n",
    "    axes[1, 1].set_ylabel(\"Loss\")\n",
    "    axes[1, 1].set_title(\"Mini-Batch GD Batch Loss\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvFUeXMNadnZ"
   },
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KThKuK5DJHAH"
   },
   "source": [
    "## Abstract Layer Class\n",
    "\n",
    "The `Layer` class serves as an abstract base class for all layers in the network. It includes placeholder methods:\n",
    "- `forward`: The forward pass computes the output of the layer given an input.\n",
    "- `backward`: The backward pass computes the gradients with respect to the input and parameters.\n",
    "- `step`: Updates the layer parameters (weights and biases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5HmznEXldEaI"
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.inp = None\n",
    "        self.out = None\n",
    "\n",
    "    def __call__(self, inp: np.ndarray) -> np.ndarray:\n",
    "        return self.forward(inp)\n",
    "\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def step(self, lr: float) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pF0DKJV_JRnh"
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        # He initialization: better scaling for deep networks\n",
    "        self.w = 0.1 * np.random.randn(in_dim, out_dim)\n",
    "        self.b = np.zeros((1, out_dim))\n",
    "        self.dw = np.zeros_like(self.w)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Perform the linear transformation: output = inp * W + b\"\"\"\n",
    "        self.inp = inp\n",
    "        self.out = np.dot(inp, self.w) + self.b\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backpropagate the gradients through this layer.\"\"\"\n",
    "        # Compute gradients for weights and biases\n",
    "        self.dw = np.dot(self.inp.T, up_grad)  # Gradient wrt weights\n",
    "        self.db = np.sum(up_grad, axis=0, keepdims=True)  # Gradient wrt biases\n",
    "        # Compute gradient to propagate back (downstream)\n",
    "        down_grad = np.dot(up_grad, self.w.T)\n",
    "        return down_grad\n",
    "\n",
    "    def step(self, lr: float) -> None:\n",
    "        \"\"\"Update the weights and biases using the gradients.\"\"\"\n",
    "        self.w -= lr * self.dw\n",
    "        self.b -= lr * self.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yr6hBcY4N00B"
   },
   "source": [
    "## Activation Functions\n",
    "\n",
    "We can implement activation functions as layers. This will simplify the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsuMui_fLCWf"
   },
   "source": [
    "### Sigmoid\n",
    "\n",
    "- The Sigmoid function is defined as follows:\n",
    "\n",
    "$$f(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "- Sigmoid squashes the input into the range [0, 1], making it useful for binary classification tasks.\n",
    "- It converts any real-valued number into a probability-like output.\n",
    "- However, in deeper networks, it may cause vanishing gradients due to its flat slope for extreme values.\n",
    "- The derivative of Sigmoid is convenient to compute using its output  $f(x)$:\n",
    "$$f'(x) = \\frac{-e^{-x}}{(1 + e^{-x})^2} = \\frac{1}{1 + e^{-x}} \\cdot \\frac{e^{-x}}{1 + e^{-x}} = f(x) \\cdot (1-f(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxGzqMybEGt-"
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Sigmoid Activation: f(x) = 1 / (1 + exp(-x))\"\"\"\n",
    "        self.out = 1 / (1 + np.exp(-inp))\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for Sigmoid: f'(x) = f(x) * (1 - f(x))\"\"\"\n",
    "        down_grad = self.out * (1 - self.out) * up_grad\n",
    "        return down_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08aibrACJ_kr"
   },
   "source": [
    "### ReLU (Rectified Linear Unit)\n",
    "\n",
    "- The ReLU function outputs 0 if the input is less than zero, otherwise it will return the input itself:\n",
    "$$f(x) = \\max(0,x) $$\n",
    "\n",
    "- ReLU helps introduce non-linearity into the model, which is essential for learning complex patterns.\n",
    "- It also helps avoid the vanishing gradient problem common in deep networks with the Sigmoid activation.\n",
    "- During backpropagation, only the gradients for inputs greater than 0 pass through:\n",
    "\n",
    "$$ f'(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ 0 & \\text{otherwise} \\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHaTiE10HART"
   },
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"ReLU Activation: f(x) = max(0, x)\"\"\"\n",
    "        self.inp = inp\n",
    "        self.out = np.maximum(0, inp)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for ReLU: derivative is 1 where input > 0, else 0.\"\"\"\n",
    "        down_grad = up_grad * (self.inp > 0)  # Efficient boolean indexing\n",
    "        return down_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CaQZUpp1coQ1"
   },
   "source": [
    "### Softmax\n",
    "\n",
    "- The Softmax function is defined as follows:\n",
    "$$f(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n",
    "\n",
    "- Softmax normalizes the input values into probabilities that sum to 1.\n",
    "- It's typically used in the final layer of a neural network for multi-class classification.\n",
    "- It converts raw scores into probabilities, where each class has a non-negative probability between 0 and 1.\n",
    "- Subtracting the maximum input value (`np.max(inp)`) from all inputs before applying `np.exp` helps prevent overflow errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wrq1L5ZCO0vM"
   },
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Softmax Activation: f(x) = exp(x) / sum(exp(x))\"\"\"\n",
    "        # Subtract max for numerical stability\n",
    "        exp_values = np.exp(inp - np.max(inp, axis=1, keepdims=True))\n",
    "        self.out = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for Softmax using the Jacobian matrix.\"\"\"\n",
    "        down_grad = np.empty_like(up_grad)\n",
    "        for i in range(up_grad.shape[0]):\n",
    "            single_output = self.out[i].reshape(-1, 1)\n",
    "            jacobian = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            down_grad[i] = np.dot(jacobian, up_grad[i])\n",
    "        return down_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sl2nW4KnSgYs"
   },
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyEmTPR-TLDk"
   },
   "source": [
    "## Abstract Loss Class\n",
    "\n",
    "The `Loss` class serves as an abstract base class for all layers in the network. It includes placeholder methods:\n",
    "- `forward`: To compute the loss given predictions and targets.\n",
    "- `backward`: To compute the loss given predictions and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMb2DsJNSp3f"
   },
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def __init__(self):\n",
    "        self.prediction = None\n",
    "        self.target = None\n",
    "        self.loss = None\n",
    "\n",
    "    def __call__(self, prediction: np.ndarray, target: np.ndarray) -> float:\n",
    "        return self.forward(prediction, target)\n",
    "\n",
    "    def forward(self, prediction: np.ndarray, target: np.ndarray) -> float:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self) -> np.ndarray:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__nX-zlJTo3B"
   },
   "source": [
    "## Cross-Entropy Loss\n",
    "\n",
    "Cross-entropy loss is typically used in classification tasks since it measures the dissimilarity between the true distribution (target) and the predicted probability distribution (prediction):\n",
    "\n",
    "$$L = - \\frac{1}{N} \\sum_{i} \\sum_{c} y_{ic} \\log(p_{ic})$$\n",
    "\n",
    "where $y_{ic}$ is the one-hot encoded true label (target), $p_{ic}$ is the predicted probability (output from Softmax) and $N$ is the batch size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lyF-rhVvSy1q"
   },
   "outputs": [],
   "source": [
    "class CrossEntropy(Loss):\n",
    "    def forward(self, prediction: np.ndarray, target: np.ndarray) -> float:\n",
    "        \"\"\"Cross-Entropy Loss for classification.\"\"\"\n",
    "        self.prediction = prediction\n",
    "        self.target = target\n",
    "        # Clip predictions to avoid log(0)\n",
    "        clipped_pred = np.clip(prediction, 1e-12, 1.0)\n",
    "        # Compute and return the loss\n",
    "        self.loss = -np.mean(np.sum(target * np.log(clipped_pred), axis=1))\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self) -> np.ndarray:\n",
    "        \"\"\"Gradient of Cross-Entropy Loss.\"\"\"\n",
    "        # Gradient wrt prediction (assuming softmax and one-hot targets)\n",
    "        grad = -self.target / self.prediction / self.target.shape[0]\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-i8pHfR9TrYa"
   },
   "source": [
    "## Mean Squared Error (MSE) Loss\n",
    "\n",
    "MSE is used primarily for regression tasks, where you need to measure the distance between the predicted continuous values and the true values:\n",
    "\n",
    "$$L = \\frac{1}{N} \\sum_{i} (p_i - y_i)^2$$\n",
    "\n",
    "where $p_i$ is the predicted value, $y_i$ is the true value (target) and $N$ is the batch size.\n",
    "\n",
    "The gradient measures the difference between the prediction and the target, scaled by the batch size:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial p_i} = \\frac{2}{N} (p_i - y_i)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ZmkwcRCSzRd"
   },
   "outputs": [],
   "source": [
    "class MSE(Loss):\n",
    "    def forward(self, prediction: np.ndarray, target: np.ndarray) -> float:\n",
    "        \"\"\"Mean Squared Error Loss for regression.\"\"\"\n",
    "        self.prediction = prediction\n",
    "        self.target = target\n",
    "        # Compute and return the loss\n",
    "        self.loss = np.mean((prediction - target) ** 2)\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self) -> np.ndarray:\n",
    "        \"\"\"Gradient of MSE Loss.\"\"\"\n",
    "        grad = 2 * (self.prediction - self.target) / self.target.size\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1KKCeDWUNJc"
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJno_CD7hK1u"
   },
   "source": [
    "Now we can combine everything we've done earlier to build a neural network class called `MLP` with the following methods:\n",
    "\n",
    "- `forward`: Sequentially passes input through each layer in the network to compute the output.\n",
    "- `loss`: Computes the loss between the predicted output and the true target using the specified loss function.\n",
    "- `backward`: Propagates the gradient from the loss function through each layer, updating the gradients of the parameters in each layer.\n",
    "- `update`: Updates each layer's parameters (e.g., weights and biases) using the gradients computed during backpropagation.\n",
    "- `train`: Executes the training loop for a specified number of epochs, iterating over the dataset in mini-batches, performing the forward pass, computing the loss, backpropagating the gradients, and updating the parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nd1HjMLMUOAs"
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, layers: list[Layer], loss_fn: Loss, lr: float) -> None:\n",
    "        \"\"\"\n",
    "        Multi-Layer Perceptron (MLP) class.\n",
    "        Arguments:\n",
    "        - layers: List of layers (e.g., Linear, ReLU, etc.).\n",
    "        - loss_fn: Loss function object (e.g., CrossEntropy, MSE).\n",
    "        - lr: Learning rate.\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.loss_fn = loss_fn\n",
    "        self.lr = lr\n",
    "\n",
    "    def __call__(self, inp: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Makes the model callable, equivalent to forward pass.\"\"\"\n",
    "        return self.forward(inp)\n",
    "\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Pass input through each layer sequentially.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            inp = layer.forward(inp)\n",
    "        return inp\n",
    "\n",
    "    def loss(self, prediction: np.ndarray, target: np.ndarray) -> float:\n",
    "        \"\"\"Calculate the loss.\"\"\"\n",
    "        return self.loss_fn(prediction, target)\n",
    "\n",
    "    def backward(self) -> None:\n",
    "        \"\"\"Perform backpropagation by propagating the gradient backwards through the layers.\"\"\"\n",
    "        up_grad = self.loss_fn.backward()\n",
    "        for layer in reversed(self.layers):\n",
    "            up_grad = layer.backward(up_grad)\n",
    "\n",
    "    def update(self) -> None:\n",
    "        \"\"\"Update the parameters of each layer using the gradients and the learning rate.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.step(self.lr)\n",
    "\n",
    "    def train(self, x_train: np.ndarray, y_train: np.ndarray, epochs: int, batch_size: int) -> np.ndarray:\n",
    "        \"\"\"Train the MLP over the given dataset for a number of epochs.\"\"\"\n",
    "        losses = np.empty(epochs)\n",
    "        for epoch in (pbar := trange(epochs)):\n",
    "            running_loss = 0.0\n",
    "            for i in range(0, len(x_train), batch_size):\n",
    "                x_batch = x_train[i:i + batch_size]\n",
    "                y_batch = y_train[i:i + batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                prediction = self.forward(x_batch)\n",
    "\n",
    "                # Compute loss\n",
    "                running_loss += self.loss(prediction, y_batch) * batch_size\n",
    "\n",
    "                # Backward pass\n",
    "                self.backward()\n",
    "\n",
    "                # Update parameters\n",
    "                self.update()\n",
    "\n",
    "            # Normalize running loss by total number of samples\n",
    "            running_loss /= len(x_train)\n",
    "            pbar.set_description(f\"Loss: {running_loss:.3f}\")\n",
    "            losses[epoch] = running_loss\n",
    "\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzONLtl5kc8W"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r93pdZdrZ77B"
   },
   "source": [
    "## Loading the Fashion-MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOCW3Kh9azY6"
   },
   "source": [
    "For simplicity you can use `get_data` to load the Fashion-MNIST dataset. Since we aren't using GPUs, in order to save time and get better results, we are only going to include 3 classes in our training. However you can easily modify this cell to include different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmRyKM-oZ-hh"
   },
   "outputs": [],
   "source": [
    "class_names = {0: 'T-shirt/top', 1: 'Trouser', 2: 'Pullover',\n",
    "               3: 'Dress', 4: 'Coat', 5:  'Sandal', 6: 'Shirt',\n",
    "               7: 'Sneaker', 8: 'Bag', 9: 'Ankle boot'}\n",
    "\n",
    "# Include all the classes you want to see in training\n",
    "kept_classes = [0, 1, 7]  # T-shirt/top, Trouser, Sneaker\n",
    "\n",
    "# Download the dataset and split it into training and testing sets\n",
    "x_train, x_test, y_train, y_test = get_data(kept_classes)\n",
    "\n",
    "# One-hot encode the target labels of the training set\n",
    "y_train = onehot_encoder(y_train, num_labels=len(kept_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZeNgQXybi4P"
   },
   "source": [
    "## Training the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qfmP-hYkpfz"
   },
   "source": [
    "Now we can define the network and train it on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "eaIADoc8W7CS",
    "outputId": "7d762b34-7b46-461d-d183-7ee9ec1862e9"
   },
   "outputs": [],
   "source": [
    "# Define the layers of the neural network\n",
    "layers = [Linear(784, 50),\n",
    "          ReLU(),\n",
    "          Linear(50, 50),\n",
    "          ReLU(),\n",
    "          Linear(50, len(kept_classes)),\n",
    "          Softmax()]\n",
    "\n",
    "# Create the model\n",
    "model = MLP(layers, CrossEntropy(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "losses = model.train(x_train, y_train, epochs=30, batch_size=64)\n",
    "\n",
    "# Plot the training loss curve\n",
    "plot_training(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4Uh-ddyk2mR"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgO7eZ9Uk5X0"
   },
   "source": [
    "We can measure the models accuracy on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3M4fWNaZoHW",
    "outputId": "d8289ad2-2775-4293-9d8c-36f32ab0764a"
   },
   "outputs": [],
   "source": [
    "# Test the model\n",
    "y_prediction = np.argmax(model(x_test), axis=1)\n",
    "acc = 100 * np.mean(y_prediction == y_test)\n",
    "print(f'Test accuracy with {len(y_train)} training examples on {len(y_test)} test samples is {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2DrYQPilBlF"
   },
   "source": [
    "The confusion matrix can also be observed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "49LwgNLMj72R",
    "outputId": "a1ae708d-967a-4526-ccb1-83041fb594bb"
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "plot_confusion_matrix(y_test, y_prediction, class_names, kept_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KIRyFvYDkvd"
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-jahzvWEVV_"
   },
   "source": [
    "In this section we are going to run some experiments to better understand the different hyperparameters of our neural network.\n",
    "We will slightly modify the `MLP` class we wrote before to access different metrics during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "I9pWMq0yHnpV"
   },
   "outputs": [],
   "source": [
    "# @title Modified MLP\n",
    "\n",
    "class NN(MLP):\n",
    "    def test(self, x_test: np.ndarray, y_test: np.ndarray) -> float:\n",
    "        \"\"\"Calculate the test accuracy and return it.\"\"\"\n",
    "        y_pred = np.argmax(self.forward(x_test), axis=1)\n",
    "        return 100 * np.mean(y_pred == y_test)\n",
    "\n",
    "    def train(self, x_train: np.ndarray, y_train: np.ndarray, epochs: int,\n",
    "              batch_size: int, x_test: np.ndarray, y_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Train the MLP over the given dataset for a number of epochs.\"\"\"\n",
    "        losses = np.zeros(epochs)\n",
    "        batch_count = len(x_train) // batch_size\n",
    "        batch_losses = np.empty(epochs * batch_count + 1)\n",
    "        accuracies = np.empty(epochs)\n",
    "\n",
    "        for epoch in (pbar := trange(epochs)):\n",
    "            correct = 0\n",
    "            for i in range(0, len(x_train), batch_size):\n",
    "                x_batch = x_train[i:i + batch_size]\n",
    "                y_batch = y_train[i:i + batch_size]\n",
    "                # Forward pass\n",
    "                prediction = self.forward(x_batch)\n",
    "                # Compute loss\n",
    "                batch_losses[i // batch_size + epoch * batch_count] = self.loss(prediction, y_batch)\n",
    "                losses[epoch] += batch_losses[i // batch_size + epoch * batch_count]\n",
    "                # Backward pass\n",
    "                self.backward()\n",
    "                # Update parameters\n",
    "                self.update()\n",
    "\n",
    "            # Display and update the metrics\n",
    "            losses[epoch] /= batch_count\n",
    "            accuracies[epoch] = self.test(x_test, y_test)\n",
    "            pbar.set_description(f\"Train Loss = {losses[epoch]:.3f} | Test Accuracy = {accuracies[epoch]:.2f}% \")\n",
    "\n",
    "        return losses, batch_losses, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KcAcyeuDuO7"
   },
   "source": [
    "## Batch Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2A_sjspETKn"
   },
   "source": [
    "Here we will take a look at different batch sizes and how they effect training and convergence. Run the widget bellow to train the model for different batch sizes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "1179f322ebf643fcbc389efa1e80f0d9",
      "b9ab8195f2d94405a822ea67ea1fd094",
      "48a192d8f8754cac918a4ea1fa60e23a"
     ]
    },
    "id": "i9JQZLyCFiG6",
    "outputId": "2c68cb95-cb03-46ce-b31a-643c9e922f02"
   },
   "outputs": [],
   "source": [
    "# @markdown Batch Size Experimentation Widget\n",
    "\n",
    "class_names = {0: 'T-shirt/top', 1: 'Trouser', 2: 'Pullover',\n",
    "               3: 'Dress', 4: 'Coat', 5:  'Sandal', 6: 'Shirt',\n",
    "               7: 'Sneaker', 8: 'Bag', 9: 'Ankle boot'}\n",
    "\n",
    "# Include all the classes you want to see in training\n",
    "kept_classes = [0, 1, 7]  # T-shirt/top, Trouser, Sneaker\n",
    "\n",
    "# Download the dataset and split it into training and testing sets\n",
    "x_train, x_test, y_train, y_test = get_data(kept_classes)\n",
    "\n",
    "# One-hot encode the target labels of the training set\n",
    "y_train = onehot_encoder(y_train, num_labels=len(kept_classes))\n",
    "\n",
    "# Create a list of values\n",
    "options = [16, 32, 64, 128, 256]\n",
    "\n",
    "# Create a dropdown widget with custom layout\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=options,\n",
    "    description='Batch Size:',\n",
    "    layout={'width': '200px'},\n",
    ")\n",
    "\n",
    "# Define a function to run based on selected value\n",
    "def on_value_change(change):\n",
    "    mini_batch_size = change['new']\n",
    "    global first_run, gd, sgd, mbgd\n",
    "    if first_run:\n",
    "        # Gradient Descent\n",
    "        layers = [Linear(784, 50), ReLU(), Linear(50, 50), ReLU(), Linear(50, len(kept_classes)), Softmax()]\n",
    "        model = NN(layers, CrossEntropy(), lr=0.001)\n",
    "        print('Gradient Descent:', end='\\n\\t')\n",
    "        gd = model.train(x_train, y_train, epochs=30, batch_size=len(x_train), x_test=x_test, y_test=y_test)\n",
    "        # Mini-Batch Gradient Descent\n",
    "        layers = [Linear(784, 50), ReLU(), Linear(50, 50), ReLU(), Linear(50, len(kept_classes)), Softmax()]\n",
    "        model = NN(layers, CrossEntropy(), lr=0.001)\n",
    "        print('Stochastic Gradient Descent:', end='\\n\\t')\n",
    "        sgd = model.train(x_train, y_train, epochs=30, batch_size=1, x_test=x_test, y_test=y_test)\n",
    "        first_run = False\n",
    "    else:\n",
    "        print('\\n')\n",
    "    # Stochastic Gradient Descent\n",
    "    layers = [Linear(784, 50), ReLU(), Linear(50, 50), ReLU(), Linear(50, len(kept_classes)), Softmax()]\n",
    "    model = NN(layers, CrossEntropy(), lr=0.001)\n",
    "    print('Mini-Batch Gradient Descent:', end='\\n\\t')\n",
    "    mbgd = model.train(x_train, y_train, epochs=30, batch_size=mini_batch_size, x_test=x_test, y_test=y_test)\n",
    "    print()\n",
    "    plot_batch_size(gd, sgd, mbgd)\n",
    "\n",
    "\n",
    "\n",
    "# Observe changes in the dropdown value\n",
    "dropdown.observe(on_value_change, names='value')\n",
    "\n",
    "# Run Vanila Gradient Descent and Stochastic Gradient Descent once\n",
    "first_run = True\n",
    "\n",
    "gd, sgd, mbgd = None, None, None\n",
    "\n",
    "# Display the widget\n",
    "display(dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpnbFt4zkgu9"
   },
   "source": [
    "As you can see, increasing the batch size improves the speed of our algorithm, while reducing the batch size allows us to achieve higher accuracies."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Ecaqr9YkWpbL"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1179f322ebf643fcbc389efa1e80f0d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DropdownModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DropdownModel",
      "_options_labels": [
       "16",
       "32",
       "64",
       "128",
       "256"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "DropdownView",
      "description": "Batch Size:",
      "description_tooltip": null,
      "disabled": false,
      "index": 4,
      "layout": "IPY_MODEL_b9ab8195f2d94405a822ea67ea1fd094",
      "style": "IPY_MODEL_48a192d8f8754cac918a4ea1fa60e23a"
     }
    },
    "48a192d8f8754cac918a4ea1fa60e23a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b9ab8195f2d94405a822ea67ea1fd094": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "200px"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

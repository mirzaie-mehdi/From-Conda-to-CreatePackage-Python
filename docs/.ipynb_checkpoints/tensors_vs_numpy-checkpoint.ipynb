{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c868719",
   "metadata": {},
   "source": [
    "# Tensors in PyTorch: A Formal Comparison with NumPy\n",
    "\n",
    "This subsection formalizes the concept of **tensors** as used in PyTorch and contrasts them rigorously with NumPy arrays. The goal is to connect the *from-scratch* implementation of neural networks to the framework-based implementation without introducing conceptual gaps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1bbba2",
   "metadata": {},
   "source": [
    "## Mathematical Definition of a Tensor\n",
    "\n",
    "In mathematics, a **tensor** is a multilinear object that generalizes scalars, vectors, and matrices.\n",
    "\n",
    "- A scalar is a tensor of order 0\n",
    "- A vector is a tensor of order 1\n",
    "- A matrix is a tensor of order 2\n",
    "- Higher-order tensors extend this idea to more dimensions\n",
    "\n",
    "Formally, a tensor $\\mathbf{T}$ of order $k$ is an element of the tensor product:\n",
    "\n",
    "$$\n",
    "\\mathbf{T} \\in V_1 \\otimes V_2 \\otimes \\cdots \\otimes V_k\n",
    "$$\n",
    "\n",
    "In numerical computing, tensors are represented as multi-dimensional arrays with fixed shapes and data types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d046ce05",
   "metadata": {},
   "source": [
    "## NumPy Arrays vs PyTorch Tensors: Conceptual Overview\n",
    "\n",
    "Although NumPy arrays and PyTorch tensors appear similar, they differ fundamentally in purpose and capability.\n",
    "\n",
    "| Feature | NumPy Array | PyTorch Tensor |\n",
    "|--------|-------------|----------------|\n",
    "| Multi-dimensional data | Yes | Yes |\n",
    "| Automatic differentiation | No | Yes |\n",
    "| GPU acceleration | No | Yes |\n",
    "| Computation graph | No | Yes |\n",
    "| Deep learning primitives | Limited | Native |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4361bd",
   "metadata": {},
   "source": [
    "## Side-by-Side Example: Forward Computation\n",
    "\n",
    "Consider a linear transformation:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{X}\\mathbf{W} + \\mathbf{b}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{X} \\in \\mathbb{R}^{N \\times d}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb16a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy implementation\n",
    "import numpy as np\n",
    "\n",
    "X = np.random.randn(4, 3)\n",
    "W = np.random.randn(3, 2)\n",
    "b = np.random.randn(2)\n",
    "\n",
    "y_numpy = X @ W + b\n",
    "y_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2f22ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch implementation\n",
    "import torch\n",
    "\n",
    "X_t = torch.randn(4, 3)\n",
    "W_t = torch.randn(3, 2)\n",
    "b_t = torch.randn(2)\n",
    "\n",
    "y_torch = X_t @ W_t + b_t\n",
    "y_torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a09c54d",
   "metadata": {},
   "source": [
    "Numerically, both implementations produce equivalent results. The distinction becomes critical when gradients are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452125b6",
   "metadata": {},
   "source": [
    "## Backpropagation: Manual vs Automatic\n",
    "\n",
    "In the from-scratch implementation, gradients were derived manually:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = X^T \\frac{\\partial L}{\\partial y}\n",
    "$$\n",
    "\n",
    "Using NumPy, this must be explicitly coded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c47ff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy: manual gradient computation\n",
    "grad_y = np.ones_like(y_numpy)\n",
    "grad_W = X.T @ grad_y\n",
    "grad_W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bd3b9a",
   "metadata": {},
   "source": [
    "PyTorch tensors record the computation graph automatically and apply the chain rule internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174fc4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch: automatic differentiation\n",
    "X_t = torch.randn(4, 3, requires_grad=True)\n",
    "W_t = torch.randn(3, 2, requires_grad=True)\n",
    "b_t = torch.randn(2, requires_grad=True)\n",
    "\n",
    "y = X_t @ W_t + b_t\n",
    "loss = y.sum()\n",
    "loss.backward()\n",
    "\n",
    "W_t.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd09629",
   "metadata": {},
   "source": [
    "## Why Automatic Differentiation Matters\n",
    "\n",
    "For deep networks, the loss function may involve millions of parameters and thousands of operations. Manual differentiation becomes error-prone and infeasible. PyTorch tensors transform gradient computation from an *implementation problem* into a *declarative problem*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade20f97",
   "metadata": {},
   "source": [
    "## Tensor Data Types and Semantics\n",
    "\n",
    "Each tensor has an associated data type (`dtype`) that determines numerical precision and valid operations.\n",
    "\n",
    "- `torch.float32`: model parameters and activations\n",
    "- `torch.int64`: class labels\n",
    "\n",
    "PyTorch enforces stricter type semantics than NumPy, which reduces silent bugs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbabda6",
   "metadata": {},
   "source": [
    "## Hardware Acceleration and Devices\n",
    "\n",
    "Every tensor is associated with a device (CPU or GPU):\n",
    "\n",
    "$$\n",
    "\\text{tensor} \\rightarrow \\{\\text{CPU}, \\text{GPU}\\}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72babdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "X_device = X_t.to(device)\n",
    "X_device.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466259d5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- NumPy arrays store numerical values\n",
    "- PyTorch tensors store numerical values **and** differentiation metadata\n",
    "- The underlying mathematics is unchanged\n",
    "- Tensors enable scalable, maintainable deep learning systems\n",
    "\n",
    "**Understanding tensors formalizes and extends the concepts developed in the from-scratch implementation.**"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

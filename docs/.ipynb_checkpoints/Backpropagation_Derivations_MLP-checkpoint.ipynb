{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22eca233",
   "metadata": {},
   "source": [
    "# Mathematical Equations, Computational Graphs, and Backpropagation\n",
    "\n",
    "## Purpose\n",
    "This notebook provides a clear, end-to-end explanation of why the **forward pass** starts at the input while the **backward pass** starts at the loss, and how gradients are propagated layer-by-layer using the chain rule.\n",
    "\n",
    "It is written to be copied directly into your documentation section titled **“Mathematical Equations, Computational Graphs, and Backpropagation.”**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b904ff7e",
   "metadata": {},
   "source": [
    "## 1. Forward vs. Backward: values vs. sensitivities\n",
    "\n",
    "A feedforward neural network defines a **composition of functions**:\n",
    "\n",
    "\\[\n",
    "X \\xrightarrow{f_1} H_1 \\xrightarrow{f_2} H_2 \\xrightarrow{} \\cdots \\xrightarrow{f_L} Z \\xrightarrow{\\text{loss}} L\n",
    "\\]\n",
    "\n",
    "- **Forward pass** computes *values* of intermediate variables \\(H_1, H_2, \\dots, Z\\) starting from the input \\(X\\).\n",
    "- **Backward pass** computes *sensitivities* (gradients) of the final scalar loss \\(L\\) with respect to intermediate variables and parameters.\n",
    "\n",
    "Because \\(L\\) is a **scalar**, gradients like \\(\\partial L/\\partial v\\) have a direct interpretation:\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial L}{\\partial v} \\quad\\text{measures how a small change in } v \\text{ would change the loss.}\n",
    "\\]\n",
    "\n",
    "This is why backpropagation naturally starts at the loss: the loss is the endpoint of the computational graph, and all parameters influence the loss only through the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4649f802",
   "metadata": {},
   "source": [
    "## 2. Computational graph and the chain rule\n",
    "\n",
    "Backpropagation is the chain rule applied systematically.\n",
    "\n",
    "If a variable \\(u\\) influences \\(v\\), and \\(v\\) influences the loss \\(L\\), then:\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial L}{\\partial u} = \\frac{\\partial L}{\\partial v} \\cdot \\frac{\\partial v}{\\partial u}\n",
    "\\]\n",
    "\n",
    "The operational principle is modular:\n",
    "\n",
    "- Each layer/module computes a forward mapping \\(v = f(u;\\theta)\\).\n",
    "- During backward, if we know the upstream gradient \\(G_{out}=\\partial L/\\partial v\\), the layer can compute:\n",
    "  - **input gradient** \\(G_{in}=\\partial L/\\partial u\\)\n",
    "  - **parameter gradients** \\(\\partial L/\\partial \\theta\\) (if the layer has parameters)\n",
    "\n",
    "This is why you can say:\n",
    "\n",
    "**Given the gradient of the next layer, we can compute the gradient of the current layer.**\n",
    "\n",
    "It is simply the chain rule written in a reusable layer-by-layer form.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0727ec6",
   "metadata": {},
   "source": [
    "## 3. MLP notation (batch form)\n",
    "\n",
    "For a batch size \\(N\\):\n",
    "\n",
    "- \\(H_0 = X \\in \\mathbb{R}^{N\\times d_0}\\)\n",
    "- For \\(\\ell=1,\\dots,L\\):\n",
    "  - Affine (Dense) transform:\n",
    "    \\[\n",
    "    Z_\\ell = H_{\\ell-1} W_\\ell + b_\\ell\n",
    "    \\quad\n",
    "    (W_\\ell \\in \\mathbb{R}^{d_{\\ell-1}\\times d_\\ell},\\; b_\\ell\\in\\mathbb{R}^{d_\\ell})\n",
    "    \\]\n",
    "  - Nonlinearity:\n",
    "    \\[\n",
    "    H_\\ell = \\phi(Z_\\ell)\n",
    "    \\]\n",
    "\n",
    "Final logits are \\(Z_L\\). The training objective is a scalar loss \\(L\\) computed from \\(Z_L\\) and labels.\n",
    "\n",
    "We want gradients for all parameters:\n",
    "\n",
    "\\[\n",
    "\\left\\{\\frac{\\partial L}{\\partial W_\\ell},\\frac{\\partial L}{\\partial b_\\ell}\\right\\}_{\\ell=1}^{L}\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6029979",
   "metadata": {},
   "source": [
    "## 4. Why backward starts from the loss\n",
    "\n",
    "The loss \\(L\\) is a scalar function of the final model output. Every parameter affects \\(L\\) only through the sequence of intermediate computations.\n",
    "\n",
    "Therefore the derivative propagation must begin at the endpoint:\n",
    "\n",
    "1. Compute \\(\\partial L/\\partial Z_L\\) (loss gradient w.r.t. logits).\n",
    "2. Propagate gradients backward through each layer:\n",
    "   \\[\n",
    "   \\frac{\\partial L}{\\partial H_{L-1}},\\; \\frac{\\partial L}{\\partial Z_{L-1}},\\; \\dots,\\; \\frac{\\partial L}{\\partial H_0}\n",
    "   \\]\n",
    "3. At each layer, compute parameter gradients \\(\\partial L/\\partial W_\\ell\\) and \\(\\partial L/\\partial b_\\ell\\).\n",
    "\n",
    "This is exactly the chain rule for a composition of functions, applied from right to left.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eef84d0",
   "metadata": {},
   "source": [
    "## 5. The reusable layer pattern\n",
    "\n",
    "Each layer exposes a backward mapping:\n",
    "\n",
    "- **Input:** upstream gradient \\(G_{out}=\\partial L/\\partial (\\text{layer output})\\)\n",
    "- **Outputs:**\n",
    "  - downstream gradient \\(G_{in}=\\partial L/\\partial (\\text{layer input})\\)\n",
    "  - parameter gradients \\(\\partial L/\\partial \\theta\\) (if parameters exist)\n",
    "\n",
    "In code, this corresponds to receiving `grad_out` and returning `grad_in`, while storing `dW`, `db`, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68607c4c",
   "metadata": {},
   "source": [
    "## 6. Loss layer: Softmax + Cross-Entropy\n",
    "\n",
    "Let logits be \\(Z\\in\\mathbb{R}^{N\\times C}\\), labels be one-hot \\(Y\\in\\mathbb{R}^{N\\times C}\\).\n",
    "\n",
    "Softmax:\n",
    "\\[\n",
    "P_{n,i} = \\frac{e^{Z_{n,i}}}{\\sum_{j=1}^{C} e^{Z_{n,j}}}\n",
    "\\]\n",
    "\n",
    "Mean cross-entropy loss:\n",
    "\\[\n",
    "L = \\frac{1}{N}\\sum_{n=1}^{N} \\left(-\\sum_{i=1}^{C} Y_{n,i}\\log P_{n,i}\\right)\n",
    "\\]\n",
    "\n",
    "A key result (from differentiating softmax and cross-entropy together) is:\n",
    "\n",
    "\\[\n",
    "\\boxed{\\frac{\\partial L}{\\partial Z} = \\frac{1}{N}(P - Y)}\n",
    "\\]\n",
    "\n",
    "This gradient is the **starting point** of backpropagation into the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca3ee32",
   "metadata": {},
   "source": [
    "## 7. Activation layer: ReLU\n",
    "\n",
    "Forward:\n",
    "\\[\n",
    "H = \\mathrm{ReLU}(Z) = \\max(0, Z)\n",
    "\\]\n",
    "\n",
    "Elementwise derivative:\n",
    "\\[\n",
    "\\frac{\\partial H}{\\partial Z} = \\mathbf{1}_{Z>0}\n",
    "\\]\n",
    "\n",
    "Given upstream gradient \\(G_H = \\partial L/\\partial H\\), chain rule yields:\n",
    "\n",
    "\\[\n",
    "\\boxed{\\frac{\\partial L}{\\partial Z} = G_H \\odot \\mathbf{1}_{Z>0}}\n",
    "\\]\n",
    "\n",
    "This shows why we cache \\(Z\\) in forward: the mask \\(\\mathbf{1}_{Z>0}\\) is computed from it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc8766b",
   "metadata": {},
   "source": [
    "## 8. Dense (Affine) layer\n",
    "\n",
    "Forward:\n",
    "\\[\n",
    "Z = HW + b\n",
    "\\]\n",
    "\n",
    "with \\(H\\in\\mathbb{R}^{N\\times d_{in}}\\), \\(W\\in\\mathbb{R}^{d_{in}\\times d_{out}}\\), \\(b\\in\\mathbb{R}^{d_{out}}\\), \\(Z\\in\\mathbb{R}^{N\\times d_{out}}\\).\n",
    "\n",
    "Let upstream gradient be \\(G_Z = \\partial L/\\partial Z\\). Then:\n",
    "\n",
    "**Weights**\n",
    "\\[\n",
    "\\boxed{\\frac{\\partial L}{\\partial W} = H^\\top G_Z}\n",
    "\\]\n",
    "\n",
    "**Bias**\n",
    "\\[\n",
    "\\boxed{\\frac{\\partial L}{\\partial b} = \\sum_{n=1}^{N} (G_Z)_{n,:}}\n",
    "\\]\n",
    "\n",
    "**Inputs (to propagate further backward)**\n",
    "\\[\n",
    "\\boxed{\\frac{\\partial L}{\\partial H} = G_Z W^\\top}\n",
    "\\]\n",
    "\n",
    "These are the core backprop equations used repeatedly throughout an MLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f312f8d",
   "metadata": {},
   "source": [
    "## 9. End-to-end example: two-layer MLP\n",
    "\n",
    "Consider:\n",
    "\\[\n",
    "X=H_0 \\rightarrow Z_1=H_0W_1+b_1 \\rightarrow H_1=\\mathrm{ReLU}(Z_1) \\rightarrow Z_2=H_1W_2+b_2 \\rightarrow P=\\mathrm{softmax}(Z_2) \\rightarrow L\n",
    "\\]\n",
    "\n",
    "Backward proceeds in reverse:\n",
    "\n",
    "1. **Loss → logits**\n",
    "   \\[\n",
    "   G_{Z_2}=\\frac{\\partial L}{\\partial Z_2}=\\frac{1}{N}(P-Y)\n",
    "   \\]\n",
    "\n",
    "2. **Dense2**\n",
    "   \\[\n",
    "   \\frac{\\partial L}{\\partial W_2}=H_1^\\top G_{Z_2},\\quad\n",
    "   \\frac{\\partial L}{\\partial b_2}=\\sum_n (G_{Z_2})_{n,:},\\quad\n",
    "   G_{H_1}=G_{Z_2}W_2^\\top\n",
    "   \\]\n",
    "\n",
    "3. **ReLU1**\n",
    "   \\[\n",
    "   G_{Z_1}=G_{H_1}\\odot \\mathbf{1}_{Z_1>0}\n",
    "   \\]\n",
    "\n",
    "4. **Dense1**\n",
    "   \\[\n",
    "   \\frac{\\partial L}{\\partial W_1}=H_0^\\top G_{Z_1},\\quad\n",
    "   \\frac{\\partial L}{\\partial b_1}=\\sum_n (G_{Z_1})_{n,:},\\quad\n",
    "   G_{H_0}=G_{Z_1}W_1^\\top\n",
    "   \\]\n",
    "\n",
    "At the end, we have gradients for every parameter and can apply an optimizer update.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dfeed5",
   "metadata": {},
   "source": [
    "## 10. Why we cache forward values\n",
    "\n",
    "Backprop rules require certain forward intermediates:\n",
    "\n",
    "- Dense needs \\(H\\) to compute \\(H^\\top G_Z\\)\n",
    "- ReLU needs \\(Z\\) to compute the mask \\(\\mathbf{1}_{Z>0}\\)\n",
    "- Softmax+CE needs \\(P\\) (or logits \\(Z\\)) and labels\n",
    "\n",
    "Therefore implementations store (cache) inputs/outputs during forward so that local derivatives can be computed efficiently during backward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad752fdc",
   "metadata": {},
   "source": [
    "## 11. Key takeaway\n",
    "\n",
    "Backpropagation is:\n",
    "\n",
    "1. Compute forward values through a computational graph.\n",
    "2. Seed the backward pass at the loss by computing \\(\\partial L/\\partial (\\text{final output})\\).\n",
    "3. Move backward one layer at a time using local chain-rule updates:\n",
    "   - given \\(\\partial L/\\partial (\\text{output})\\), compute \\(\\partial L/\\partial (\\text{input})\\) and \\(\\partial L/\\partial (\\text{parameters})\\)\n",
    "\n",
    "This is why, once you have gradients from the next layer, you can compute gradients for the current layer.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
